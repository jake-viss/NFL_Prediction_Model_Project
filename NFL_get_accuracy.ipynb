{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably going to have to install this if you haven't already\n",
    "from sportsreference.nfl.boxscore import Boxscores, Boxscore\n",
    "\n",
    "# the usual imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The game_Data function is extracting game statistics for each game. It loops through each game and each week grabbing the statistics referenced.\n",
    "# This function creates the foundation for our final dataset.\n",
    "def game_data(game_df,game_stats):\n",
    "    try:\n",
    "        # Creates a dataframe for the away_team and the home_team. Sets column names to be exact matches between the two.\n",
    "        away_team_df = game_df[['away_name', 'away_abbr', 'away_score']].rename(columns = {'away_name': 'team_name', 'away_abbr': 'team_abbr', 'away_score': 'score'})\n",
    "        home_team_df = game_df[['home_name','home_abbr', 'home_score']].rename(columns = {'home_name': 'team_name', 'home_abbr': 'team_abbr', 'home_score': 'score'})\n",
    "        try:\n",
    "            if game_df.loc[0,'away_score'] > game_df.loc[0,'home_score']:\n",
    "                away_team_df = pd.merge(away_team_df, pd.DataFrame({'game_won' : [1], 'game_lost' : [0]}),left_index = True, right_index = True)\n",
    "                home_team_df = pd.merge(home_team_df, pd.DataFrame({'game_won' : [0], 'game_lost' : [1]}),left_index = True, right_index = True)\n",
    "            elif game_df.loc[0,'away_score'] < game_df.loc[0,'home_score']:\n",
    "                away_team_df = pd.merge(away_team_df, pd.DataFrame({'game_won' : [0], 'game_lost' : [1]}),left_index = True, right_index = True)\n",
    "                home_team_df = pd.merge(home_team_df, pd.DataFrame({'game_won' : [1], 'game_lost' : [0]}),left_index = True, right_index = True)\n",
    "            else: \n",
    "                away_team_df = pd.merge(away_team_df, pd.DataFrame({'game_won' : [0], 'game_lost' : [0]}),left_index = True, right_index = True)\n",
    "                home_team_df = pd.merge(home_team_df, pd.DataFrame({'game_won' : [0], 'game_lost' : [0]}),left_index = True, right_index = True)\n",
    "        except TypeError:\n",
    "                away_team_df = pd.merge(away_team_df, pd.DataFrame({'game_won' : [np.nan], 'game_lost' : [np.nan]}),left_index = True, right_index = True)\n",
    "                home_team_df = pd.merge(home_team_df, pd.DataFrame({'game_won' : [np.nan], 'game_lost' : [np.nan]}),left_index = True, right_index = True)        \n",
    "\n",
    "        # Creating the away_team & home_team stats dataframe. Grabbing the selected stats and then renaming them to match home == away dataframe column names.\n",
    "        away_stats_df = game_stats.dataframe[['away_first_downs', 'away_fourth_down_attempts',\n",
    "               'away_fourth_down_conversions', 'away_fumbles', 'away_fumbles_lost',\n",
    "               'away_interceptions', 'away_net_pass_yards', 'away_pass_attempts',\n",
    "               'away_pass_completions', 'away_pass_touchdowns', 'away_pass_yards',\n",
    "               'away_penalties', 'away_points', 'away_rush_attempts',\n",
    "               'away_rush_touchdowns', 'away_rush_yards', 'away_third_down_attempts',\n",
    "               'away_third_down_conversions', 'away_time_of_possession',\n",
    "               'away_times_sacked', 'away_total_yards', 'away_turnovers',\n",
    "               'away_yards_from_penalties', 'away_yards_lost_from_sacks']].reset_index().drop(columns ='index').rename(columns = {\n",
    "               'away_first_downs': 'first_downs', 'away_fourth_down_attempts':'fourth_down_attempts',\n",
    "               'away_fourth_down_conversions':'fourth_down_conversions' , 'away_fumbles': 'fumbles', 'away_fumbles_lost': 'fumbles_lost',\n",
    "               'away_interceptions': 'interceptions', 'away_net_pass_yards':'net_pass_yards' , 'away_pass_attempts': 'pass_attempts',\n",
    "               'away_pass_completions':'pass_completions' , 'away_pass_touchdowns': 'pass_touchdowns', 'away_pass_yards': 'pass_yards',\n",
    "               'away_penalties': 'penalties', 'away_points': 'points', 'away_rush_attempts': 'rush_attempts',\n",
    "               'away_rush_touchdowns': 'rush_touchdowns', 'away_rush_yards': 'rush_yards', 'away_third_down_attempts': 'third_down_attempts',\n",
    "               'away_third_down_conversions': 'third_down_conversions', 'away_time_of_possession': 'time_of_possession',\n",
    "               'away_times_sacked': 'times_sacked', 'away_total_yards': 'total_yards', 'away_turnovers': 'turnovers',\n",
    "               'away_yards_from_penalties':'yards_from_penalties', 'away_yards_lost_from_sacks': 'yards_lost_from_sacks'})\n",
    "\n",
    "        home_stats_df = game_stats.dataframe[['home_first_downs', 'home_fourth_down_attempts',\n",
    "               'home_fourth_down_conversions', 'home_fumbles', 'home_fumbles_lost',\n",
    "               'home_interceptions', 'home_net_pass_yards', 'home_pass_attempts',\n",
    "               'home_pass_completions', 'home_pass_touchdowns', 'home_pass_yards',\n",
    "               'home_penalties', 'home_points', 'home_rush_attempts',\n",
    "               'home_rush_touchdowns', 'home_rush_yards', 'home_third_down_attempts',\n",
    "               'home_third_down_conversions', 'home_time_of_possession',\n",
    "               'home_times_sacked', 'home_total_yards', 'home_turnovers',\n",
    "               'home_yards_from_penalties', 'home_yards_lost_from_sacks']].reset_index().drop(columns = 'index').rename(columns = {\n",
    "               'home_first_downs': 'first_downs', 'home_fourth_down_attempts':'fourth_down_attempts',\n",
    "               'home_fourth_down_conversions':'fourth_down_conversions' , 'home_fumbles': 'fumbles', 'home_fumbles_lost': 'fumbles_lost',\n",
    "               'home_interceptions': 'interceptions', 'home_net_pass_yards':'net_pass_yards' , 'home_pass_attempts': 'pass_attempts',\n",
    "               'home_pass_completions':'pass_completions' , 'home_pass_touchdowns': 'pass_touchdowns', 'home_pass_yards': 'pass_yards',\n",
    "               'home_penalties': 'penalties', 'home_points': 'points', 'home_rush_attempts': 'rush_attempts',\n",
    "               'home_rush_touchdowns': 'rush_touchdowns', 'home_rush_yards': 'rush_yards', 'home_third_down_attempts': 'third_down_attempts',\n",
    "               'home_third_down_conversions': 'third_down_conversions', 'home_time_of_possession': 'time_of_possession',\n",
    "               'home_times_sacked': 'times_sacked', 'home_total_yards': 'total_yards', 'home_turnovers': 'turnovers',\n",
    "               'home_yards_from_penalties':'yards_from_penalties', 'home_yards_lost_from_sacks': 'yards_lost_from_sacks'})\n",
    "        \n",
    "        # Merge the team_df & stats_df for both home & away teams. Set the left_index & right_index to True so that both dataframes merge on the same indices. \n",
    "        away_team_df = pd.merge(away_team_df, away_stats_df,left_index = True, right_index = True)\n",
    "        home_team_df = pd.merge(home_team_df, home_stats_df,left_index = True, right_index = True)\n",
    "        try:\n",
    "            # Converting time_of_possession from MM:SS format into seconds(int). \n",
    "            away_team_df['time_of_possession'] = (int(away_team_df['time_of_possession'].loc[0][0:2]) * 60) + int(away_team_df['time_of_possession'].loc[0][3:5])\n",
    "            home_team_df['time_of_possession'] = (int(home_team_df['time_of_possession'].loc[0][0:2]) * 60) + int(home_team_df['time_of_possession'].loc[0][3:5])\n",
    "        except TypeError:\n",
    "            away_team_df['time_of_possession'] = np.nan\n",
    "            home_team_df['time_of_possession'] = np.nan\n",
    "    except TypeError:\n",
    "        away_team_df = pd.DataFrame()\n",
    "        home_team_df = pd.DataFrame()\n",
    "    return away_team_df, home_team_df\n",
    "\n",
    "def game_data_up_to_week(weeks,year):\n",
    "    weeks_games_df = pd.DataFrame()\n",
    "    for w in range(len(weeks)):\n",
    "        date_string = str(weeks[w]) + '-' + str(year)\n",
    "        week_scores = Boxscores(weeks[w],year)\n",
    "        week_games_df = pd.DataFrame()\n",
    "        for g in range(len(week_scores.games[date_string])):\n",
    "            game_str = week_scores.games[date_string][g]['boxscore']\n",
    "            game_stats = Boxscore(game_str)\n",
    "            game_df = pd.DataFrame(week_scores.games[date_string][g], index = [0])\n",
    "            away_team_df, home_team_df = game_data(game_df,game_stats)\n",
    "            away_team_df['week'] = weeks[w]\n",
    "            home_team_df['week'] = weeks[w]\n",
    "            week_games_df = pd.concat([week_games_df,away_team_df])\n",
    "            week_games_df = pd.concat([week_games_df,home_team_df])\n",
    "        weeks_games_df = pd.concat([weeks_games_df,week_games_df])\n",
    "    return weeks_games_df\n",
    "\n",
    "def get_schedule(year):\n",
    "    weeks = list(range(1,18))\n",
    "    schedule_df = pd.DataFrame()\n",
    "    for w in range(len(weeks)):\n",
    "        date_string = str(weeks[w]) + '-' + str(year)\n",
    "        week_scores = Boxscores(weeks[w],year)\n",
    "        week_games_df = pd.DataFrame()\n",
    "        for g in range(len(week_scores.games[date_string])):\n",
    "            game = pd.DataFrame(week_scores.games[date_string][g], index = [0])[['away_name', 'away_abbr','home_name', 'home_abbr','winning_name', 'winning_abbr' ]]\n",
    "            game['week'] = weeks[w]\n",
    "            week_games_df = pd.concat([week_games_df,game])\n",
    "        schedule_df = pd.concat([schedule_df, week_games_df]).reset_index().drop(columns = 'index') \n",
    "    return schedule_df \n",
    "\n",
    "def agg_weekly_data(schedule_df,weeks_games_df,current_week,weeks):\n",
    "    schedule_df = schedule_df[schedule_df.week < current_week]\n",
    "    agg_games_df = pd.DataFrame()\n",
    "    for w in range(1,len(weeks)):\n",
    "        games_df = schedule_df[schedule_df.week == weeks[w]]\n",
    "        agg_weekly_df = weeks_games_df[weeks_games_df.week < weeks[w]].drop(columns = ['score','week','game_won', 'game_lost']).groupby(by=[\"team_name\", \"team_abbr\"]).mean().reset_index()\n",
    "        win_loss_df = weeks_games_df[weeks_games_df.week < weeks[w]][[\"team_name\", \"team_abbr\",'game_won', 'game_lost']].groupby(by=[\"team_name\", \"team_abbr\"]).sum().reset_index()\n",
    "        win_loss_df['win_perc'] = win_loss_df['game_won'] / (win_loss_df['game_won'] + win_loss_df['game_lost'])\n",
    "        win_loss_df = win_loss_df.drop(columns = ['game_won', 'game_lost'])\n",
    "\n",
    "        try:\n",
    "            agg_weekly_df['fourth_down_perc'] = agg_weekly_df['fourth_down_conversions'] / agg_weekly_df['fourth_down_attempts']  \n",
    "        except ZeroDivisionError:\n",
    "            agg_weekly_df['fourth_down_perc'] = 0 \n",
    "        agg_weekly_df['fourth_down_perc'] = agg_weekly_df['fourth_down_perc'].fillna(0)\n",
    "\n",
    "        try:\n",
    "            agg_weekly_df['third_down_perc'] = agg_weekly_df['third_down_conversions'] / agg_weekly_df['third_down_attempts']  \n",
    "        except ZeroDivisionError:\n",
    "            agg_weekly_df['third_down_perc'] = 0\n",
    "        agg_weekly_df['third_down_perc'] = agg_weekly_df['third_down_perc'].fillna(0)  \n",
    "\n",
    "        agg_weekly_df = agg_weekly_df.drop(columns = ['fourth_down_attempts', 'fourth_down_conversions', 'third_down_attempts', 'third_down_conversions'])\n",
    "        agg_weekly_df = pd.merge(win_loss_df,agg_weekly_df,left_on = ['team_name', 'team_abbr'], right_on = ['team_name', 'team_abbr'])\n",
    "\n",
    "        away_df = pd.merge(games_df,agg_weekly_df,how = 'inner', left_on = ['away_name', 'away_abbr'], right_on = ['team_name', 'team_abbr']).drop(columns = ['team_name', 'team_abbr']).rename(columns = {\n",
    "                'win_perc': 'away_win_perc',\n",
    "               'first_downs': 'away_first_downs', 'fumbles': 'away_fumbles', 'fumbles_lost':'away_fumbles_lost', 'interceptions':'away_interceptions',\n",
    "               'net_pass_yards': 'away_net_pass_yards', 'pass_attempts':'away_pass_attempts', 'pass_completions':'away_pass_completions',\n",
    "               'pass_touchdowns':'away_pass_touchdowns', 'pass_yards':'away_pass_yards', 'penalties':'away_penalties', 'points':'away_points', 'rush_attempts':'away_rush_attempts',\n",
    "               'rush_touchdowns':'away_rush_touchdowns', 'rush_yards':'away_rush_yards', 'time_of_possession':'away_time_of_possession', 'times_sacked':'away_times_sacked',\n",
    "               'total_yards':'away_total_yards', 'turnovers':'away_turnovers', 'yards_from_penalties':'away_yards_from_penalties',\n",
    "               'yards_lost_from_sacks': 'away_yards_lost_from_sacks', 'fourth_down_perc':'away_fourth_down_perc', 'third_down_perc':'away_third_down_perc'})\n",
    "\n",
    "        home_df = pd.merge(games_df,agg_weekly_df,how = 'inner', left_on = ['home_name', 'home_abbr'], right_on = ['team_name', 'team_abbr']).drop(columns = ['team_name', 'team_abbr']).rename(columns = {\n",
    "                'win_perc': 'home_win_perc',\n",
    "               'first_downs': 'home_first_downs', 'fumbles': 'home_fumbles', 'fumbles_lost':'home_fumbles_lost', 'interceptions':'home_interceptions',\n",
    "               'net_pass_yards': 'home_net_pass_yards', 'pass_attempts':'home_pass_attempts', 'pass_completions':'home_pass_completions',\n",
    "               'pass_touchdowns':'home_pass_touchdowns', 'pass_yards':'home_pass_yards', 'penalties':'home_penalties', 'points':'home_points', 'rush_attempts':'home_rush_attempts',\n",
    "               'rush_touchdowns':'home_rush_touchdowns', 'rush_yards':'home_rush_yards', 'time_of_possession':'home_time_of_possession', 'times_sacked':'home_times_sacked',\n",
    "               'total_yards':'home_total_yards', 'turnovers':'home_turnovers', 'yards_from_penalties':'home_yards_from_penalties',\n",
    "               'yards_lost_from_sacks': 'home_yards_lost_from_sacks', 'fourth_down_perc':'home_fourth_down_perc', 'third_down_perc':'home_third_down_perc'})\n",
    "\n",
    "        agg_weekly_df = pd.merge(away_df,home_df,left_on = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'winning_name',\n",
    "               'winning_abbr', 'week'], right_on = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'winning_name',\n",
    "               'winning_abbr', 'week'])\n",
    "\n",
    "        agg_weekly_df['win_perc_dif'] = agg_weekly_df['away_win_perc'] - agg_weekly_df['home_win_perc']\n",
    "        agg_weekly_df['first_downs_dif'] = agg_weekly_df['away_first_downs'] - agg_weekly_df['home_first_downs']\n",
    "        agg_weekly_df['fumbles_dif'] = agg_weekly_df['away_fumbles'] - agg_weekly_df['home_fumbles']\n",
    "        agg_weekly_df['interceptions_dif'] = agg_weekly_df['away_interceptions'] - agg_weekly_df['home_interceptions']\n",
    "        agg_weekly_df['net_pass_yards_dif'] = agg_weekly_df['away_net_pass_yards'] - agg_weekly_df['home_net_pass_yards']\n",
    "        agg_weekly_df['pass_attempts_dif'] = agg_weekly_df['away_pass_attempts'] - agg_weekly_df['home_pass_attempts']\n",
    "        agg_weekly_df['pass_completions_dif'] = agg_weekly_df['away_pass_completions'] - agg_weekly_df['home_pass_completions']\n",
    "        agg_weekly_df['pass_touchdowns_dif'] = agg_weekly_df['away_pass_touchdowns'] - agg_weekly_df['home_pass_touchdowns']\n",
    "        agg_weekly_df['pass_yards_dif'] = agg_weekly_df['away_pass_yards'] - agg_weekly_df['home_pass_yards']\n",
    "        agg_weekly_df['penalties_dif'] = agg_weekly_df['away_penalties'] - agg_weekly_df['home_penalties']\n",
    "        agg_weekly_df['points_dif'] = agg_weekly_df['away_points'] - agg_weekly_df['home_points']\n",
    "        agg_weekly_df['rush_attempts_dif'] = agg_weekly_df['away_rush_attempts'] - agg_weekly_df['home_rush_attempts']\n",
    "        agg_weekly_df['rush_touchdowns_dif'] = agg_weekly_df['away_rush_touchdowns'] - agg_weekly_df['home_rush_touchdowns']\n",
    "        agg_weekly_df['rush_yards_dif'] = agg_weekly_df['away_rush_yards'] - agg_weekly_df['home_rush_yards']\n",
    "        agg_weekly_df['time_of_possession_dif'] = agg_weekly_df['away_time_of_possession'] - agg_weekly_df['home_time_of_possession']\n",
    "        agg_weekly_df['times_sacked_dif'] = agg_weekly_df['away_times_sacked'] - agg_weekly_df['home_times_sacked']\n",
    "        agg_weekly_df['total_yards_dif'] = agg_weekly_df['away_total_yards'] - agg_weekly_df['home_total_yards']\n",
    "        agg_weekly_df['turnovers_dif'] = agg_weekly_df['away_turnovers'] - agg_weekly_df['home_turnovers']\n",
    "        agg_weekly_df['yards_from_penalties_dif'] = agg_weekly_df['away_yards_from_penalties'] - agg_weekly_df['home_yards_from_penalties']\n",
    "        agg_weekly_df['yards_lost_from_sacks_dif'] = agg_weekly_df['away_yards_lost_from_sacks'] - agg_weekly_df['home_yards_lost_from_sacks']\n",
    "        agg_weekly_df['fourth_down_perc_dif'] = agg_weekly_df['away_fourth_down_perc'] - agg_weekly_df['home_fourth_down_perc']\n",
    "        agg_weekly_df['third_down_perc_dif'] = agg_weekly_df['away_third_down_perc'] - agg_weekly_df['home_third_down_perc']\n",
    "\n",
    "        agg_weekly_df = agg_weekly_df.drop(columns = ['away_win_perc',\n",
    "               'away_first_downs', 'away_fumbles', 'away_fumbles_lost', 'away_interceptions',\n",
    "               'away_net_pass_yards', 'away_pass_attempts','away_pass_completions',\n",
    "               'away_pass_touchdowns', 'away_pass_yards', 'away_penalties', 'away_points', 'away_rush_attempts',\n",
    "               'away_rush_touchdowns', 'away_rush_yards', 'away_time_of_possession', 'away_times_sacked',\n",
    "               'away_total_yards', 'away_turnovers', 'away_yards_from_penalties',\n",
    "               'away_yards_lost_from_sacks','away_fourth_down_perc', 'away_third_down_perc','home_win_perc',\n",
    "               'home_first_downs', 'home_fumbles', 'home_fumbles_lost', 'home_interceptions',\n",
    "               'home_net_pass_yards', 'home_pass_attempts','home_pass_completions',\n",
    "               'home_pass_touchdowns', 'home_pass_yards', 'home_penalties', 'home_points', 'home_rush_attempts',\n",
    "               'home_rush_touchdowns', 'home_rush_yards', 'home_time_of_possession', 'home_times_sacked',\n",
    "               'home_total_yards', 'home_turnovers', 'home_yards_from_penalties',\n",
    "               'home_yards_lost_from_sacks','home_fourth_down_perc', 'home_third_down_perc'])\n",
    "        \n",
    "        if (agg_weekly_df['winning_name'].isnull().values.any() and weeks[w] > 3):\n",
    "            agg_weekly_df['result'] = np.nan\n",
    "            print(f\"Week {weeks[w]} games have not finished yet.\")\n",
    "        else:\n",
    "            agg_weekly_df['result'] = agg_weekly_df['winning_name'] == agg_weekly_df['away_name']\n",
    "            agg_weekly_df['result'] = agg_weekly_df['result'].astype('float')\n",
    "        agg_weekly_df = agg_weekly_df.drop(columns = ['winning_name', 'winning_abbr'])\n",
    "        agg_games_df = pd.concat([agg_games_df, agg_weekly_df])\n",
    "    agg_games_df = agg_games_df.reset_index().drop(columns = 'index')\n",
    "    # What is .drop(index = 20) doing?\n",
    "    agg_games_df = agg_games_df.drop(index = 20, axis=0)\n",
    "    return agg_games_df\n",
    "\n",
    "def get_elo():\n",
    "    elo_df = pd.read_csv('https://projects.fivethirtyeight.com/nfl-api/nfl_elo_latest.csv')\n",
    "    elo_df = elo_df.drop(columns = ['season','neutral' ,'playoff', 'elo_prob1', 'elo_prob2', 'elo1_post', 'elo2_post',\n",
    "           'qbelo1_pre', 'qbelo2_pre', 'qb1', 'qb2', 'qb1_adj', 'qb2_adj', 'qbelo_prob1', 'qbelo_prob2',\n",
    "           'qb1_game_value', 'qb2_game_value', 'qb1_value_post', 'qb2_value_post',\n",
    "           'qbelo1_post', 'qbelo2_post', 'score1', 'score2', 'quality', 'importance', 'total_rating'])\n",
    "    elo_df.date = pd.to_datetime(elo_df.date)\n",
    "    elo_df = elo_df[elo_df.date < '01-05-2022']\n",
    "\n",
    "    elo_df['team1'] = elo_df['team1'].replace(['KC', 'JAX', 'CAR', 'BAL', 'BUF', 'MIN', 'DET', 'ATL', 'NE', 'WSH',\n",
    "           'CIN', 'NO', 'SF', 'LAR', 'NYG', 'DEN', 'CLE', 'IND', 'TEN', 'NYJ',\n",
    "           'TB', 'MIA', 'PIT', 'PHI', 'GB', 'CHI', 'DAL', 'ARI', 'LAC', 'HOU',\n",
    "           'SEA', 'OAK'],\n",
    "            ['kan','jax','car', 'rav', 'buf', 'min', 'det', 'atl', 'nwe', 'was', \n",
    "            'cin', 'nor', 'sfo', 'ram', 'nyg', 'den', 'cle', 'clt', 'oti', 'nyj', \n",
    "             'tam','mia', 'pit', 'phi', 'gnb', 'chi', 'dal', 'crd', 'sdg', 'htx', 'sea', 'rai' ])\n",
    "    elo_df['team2'] = elo_df['team2'].replace(['KC', 'JAX', 'CAR', 'BAL', 'BUF', 'MIN', 'DET', 'ATL', 'NE', 'WSH',\n",
    "           'CIN', 'NO', 'SF', 'LAR', 'NYG', 'DEN', 'CLE', 'IND', 'TEN', 'NYJ',\n",
    "           'TB', 'MIA', 'PIT', 'PHI', 'GB', 'CHI', 'DAL', 'ARI', 'LAC', 'HOU',\n",
    "           'SEA', 'OAK'],\n",
    "            ['kan','jax','car', 'rav', 'buf', 'min', 'det', 'atl', 'nwe', 'was', \n",
    "            'cin', 'nor', 'sfo', 'ram', 'nyg', 'den', 'cle', 'clt', 'oti', 'nyj', \n",
    "             'tam','mia', 'pit', 'phi', 'gnb', 'chi', 'dal', 'crd', 'sdg', 'htx', 'sea', 'rai' ])\n",
    "    return elo_df\n",
    "\n",
    "def merge_rankings(agg_games_df,elo_df):\n",
    "    agg_games_df = pd.merge(agg_games_df, elo_df, how = 'inner', left_on = ['home_abbr', 'away_abbr'], right_on = ['team1', 'team2']).drop(columns = ['date','team1', 'team2'])\n",
    "    agg_games_df['elo_dif'] = agg_games_df['elo2_pre'] - agg_games_df['elo1_pre']\n",
    "    agg_games_df['qb_dif'] = agg_games_df['qb2_value_pre'] - agg_games_df['qb1_value_pre']\n",
    "    agg_games_df = agg_games_df.drop(columns = ['elo1_pre', 'elo2_pre', 'qb1_value_pre', 'qb2_value_pre'])\n",
    "    return agg_games_df\n",
    "\n",
    "def prep_test_train(current_week,weeks,year):\n",
    "    current_week = current_week + 1\n",
    "    schedule_df  = get_schedule(year)\n",
    "    weeks_games_df = game_data_up_to_week(weeks,year)\n",
    "    agg_games_df = agg_weekly_data(schedule_df,weeks_games_df,current_week,weeks)\n",
    "    elo_df = get_elo()\n",
    "    agg_games_df = merge_rankings(agg_games_df, elo_df)\n",
    "    train_df = agg_games_df[agg_games_df.result.notna()]\n",
    "    current_week = current_week - 1\n",
    "    test_df = agg_games_df[agg_games_df.week == current_week]\n",
    "    return test_df, train_df\n",
    "\n",
    "def display(y_pred,X_test):\n",
    "    for g in range(len(y_pred)):\n",
    "        win_prob = round(y_pred[g],2)\n",
    "        away_team = X_test.reset_index().drop(columns = 'index').loc[g,'away_name']\n",
    "        home_team = X_test.reset_index().drop(columns = 'index').loc[g,'home_name']\n",
    "        print(f'The {away_team} have a probability of {win_prob} of beating the {home_team}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the data for 2021, up to the current week 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week 10 games have not finished yet.\n",
      "Week 13 games have not finished yet.\n"
     ]
    }
   ],
   "source": [
    "# this step takes about five minutes to run \n",
    "current_week = 13\n",
    "weeks = list(range(1,current_week + 1))\n",
    "year = 2021\n",
    "\n",
    "pred_games_df, comp_games_df = prep_test_train(current_week,weeks,year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out 2021 info, so I don't have to load it all the time\n",
    "df = pd.concat([comp_games_df, pred_games_df], axis=0)\n",
    "df.to_csv(\"2021_week_2_through_13.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the full dataframe in and drop an unneeded column\n",
    "df = pd.read_csv('2021_week_2_through_13.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little test area\n",
    "Might remove this as this is just to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_week = 5\n",
    "comp_games_df = df[df['week'] < pred_week]\n",
    "pred_games_df = df[df['week'] == pred_week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = comp_games_df\n",
    "test_df = pred_games_df\n",
    "\n",
    "X_train = train_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "y_train = train_df[['result']] \n",
    "X_test = test_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "y_test = test_df[['result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    }
   ],
   "source": [
    "# Logistic Model\n",
    "clf = LogisticRegression(penalty='l1', dual=False, tol=0.001, C=1.0, fit_intercept=True, \n",
    "                   intercept_scaling=1, class_weight='balanced', random_state=None, \n",
    "                   solver='liblinear', max_iter=1000, multi_class='ovr', verbose=0)\n",
    "\n",
    "clf.fit(X_train_scaled, np.ravel(y_train.values))\n",
    "y_pred = clf.predict_proba(X_test_scaled)\n",
    "y_pred = y_pred[:,1]\n",
    "print(accuracy_score(y_test,np.round(y_pred)))\n",
    "#display(y_pred,test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Logistic Regression Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a for loop to the get accuracy for each week\n",
    "def accuracy_score_Log_Reg(df):\n",
    "    \n",
    "    accuracy = pd.DataFrame(columns=['week', 'Log_Reg Accuracy'])\n",
    "    \n",
    "    for w in df['week'].unique()[1:-1]:\n",
    "        train_df = df[df['week'] < w]\n",
    "        test_df = df[df['week'] == w]\n",
    "        \n",
    "        X_train = train_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "        y_train = train_df[['result']] \n",
    "        X_test = test_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "        y_test = test_df[['result']]\n",
    "        \n",
    "        clf = LogisticRegression(penalty='l1', dual=False, tol=0.001, C=1.0, fit_intercept=True, \n",
    "                   intercept_scaling=1, class_weight='balanced', random_state=None, \n",
    "                   solver='liblinear', max_iter=1000, multi_class='ovr', verbose=0)\n",
    "\n",
    "        clf.fit(X_train, np.ravel(y_train.values))\n",
    "        y_pred = clf.predict_proba(X_test)\n",
    "        y_pred = y_pred[:,1]\n",
    "        \n",
    "        accuracy.loc[w,:] = [w, accuracy_score(y_test,np.round(y_pred))]\n",
    "        \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>Log_Reg Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   week Log_Reg Accuracy\n",
       "3     3              0.6\n",
       "4     4           0.6875\n",
       "5     5              0.5\n",
       "6     6         0.857143\n",
       "7     7         0.769231\n",
       "8     8         0.466667\n",
       "9     9         0.642857\n",
       "11   11         0.666667\n",
       "12   12         0.666667"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the accuracy function\n",
    "unscaled = accuracy_score_Log_Reg(df)\n",
    "unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Logistic Regression on Scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a for loop to the get accuracy for each week\n",
    "def accuracy_score_Scaled_Log_Reg(df):\n",
    "    \n",
    "    accuracy = pd.DataFrame(columns=['week', 'Scaled_Log_Reg Accuracy'])\n",
    "    \n",
    "    for w in df['week'].unique()[1:-1]:\n",
    "        train_df = df[df['week'] < w]\n",
    "        test_df = df[df['week'] == w]\n",
    "        \n",
    "        X_train = train_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "        y_train = train_df[['result']] \n",
    "        X_test = test_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "        y_test = test_df[['result']]\n",
    "        \n",
    "        # Create a StandardScaler instance\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Fit the scaler to the features training dataset\n",
    "        X_scaler = scaler.fit(X_train)\n",
    "\n",
    "        # Fit the scaler to the features training dataset\n",
    "        X_train_scaled = X_scaler.transform(X_train)\n",
    "        X_test_scaled = X_scaler.transform(X_test)\n",
    "        \n",
    "        clf = LogisticRegression(penalty='l1', dual=False, tol=0.001, C=1.0, fit_intercept=True, \n",
    "                   intercept_scaling=1, class_weight='balanced', random_state=None, \n",
    "                   solver='liblinear', max_iter=1000, multi_class='ovr', verbose=0)\n",
    "\n",
    "        clf.fit(X_train_scaled, np.ravel(y_train.values))\n",
    "        y_pred = clf.predict_proba(X_test_scaled)\n",
    "        y_pred = y_pred[:,1]\n",
    "        \n",
    "        accuracy.loc[w,:] = [w, accuracy_score(y_test,np.round(y_pred))]\n",
    "        \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>Scaled_Log_Reg Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   week Scaled_Log_Reg Accuracy\n",
       "3     3                0.466667\n",
       "4     4                  0.6875\n",
       "5     5                   0.625\n",
       "6     6                0.857143\n",
       "7     7                0.769231\n",
       "8     8                0.466667\n",
       "9     9                0.714286\n",
       "11   11                0.733333\n",
       "12   12                0.666667"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the accuracy function\n",
    "scaled = accuracy_score_Scaled_Log_Reg(df)\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>Log_Reg Accuracy</th>\n",
       "      <th>Scaled_Log_Reg Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   week Log_Reg Accuracy Scaled_Log_Reg Accuracy\n",
       "3     3              0.6                0.466667\n",
       "4     4           0.6875                  0.6875\n",
       "5     5              0.5                   0.625\n",
       "6     6         0.857143                0.857143\n",
       "7     7         0.769231                0.769231\n",
       "8     8         0.466667                0.466667\n",
       "9     9         0.642857                0.714286\n",
       "11   11         0.666667                0.733333\n",
       "12   12         0.666667                0.666667"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([unscaled, scaled['Scaled_Log_Reg Accuracy']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = tf.keras.models.load_model(\"Resources/model_nn_2021_1_12_not_scaled_relu_relu_sigmoid_loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']_1000 epochs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Dell\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1366, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Dell\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1356, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Dell\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\Dell\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1303, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\Dell\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Dell\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 22), found shape=(None, 24)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-8b4cd28d4557>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Dell\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1366, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Dell\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1356, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Dell\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\Dell\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\training.py\", line 1303, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\Dell\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Dell\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 22), found shape=(None, 24)\n"
     ]
    }
   ],
   "source": [
    "nn.evaluate(X_test, y_test, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1989 - accuracy: 0.6154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6153846383094788"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "nn.evaluate(X_test, y_test)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "#print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "model_loss\n",
    "model_accuracy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a for loop to the get accuracy for each week\n",
    "def accuracy_score_Tensorflow(df):\n",
    "    \n",
    "    accuracy = pd.DataFrame(columns=['week', 'Tensorflow Accuracy'])\n",
    "    \n",
    "    for w in df['week'].unique()[1:-1]:\n",
    "        train_df = df[df['week'] < w]\n",
    "        test_df = df[df['week'] == w]\n",
    "        \n",
    "        X_train = train_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "        y_train = train_df[['result']] \n",
    "        X_test = test_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "        y_test = test_df[['result']]\n",
    "        \n",
    "#         # Create a StandardScaler instance\n",
    "#         scaler = StandardScaler()\n",
    "\n",
    "#         # Fit the scaler to the features training dataset\n",
    "#         X_scaler = scaler.fit(X_train)\n",
    "\n",
    "#         # Fit the scaler to the features training dataset\n",
    "#         X_train_scaled = X_scaler.transform(X_train)\n",
    "#         X_test_scaled = X_scaler.transform(X_test)\n",
    "        \n",
    "        # Evaluate the model loss and accuracy metrics using the evaluate method and the test data        \n",
    "        model_accuracy = nn.evaluate(X_test, y_test)\n",
    "    \n",
    "        # Assign the accuracy to \n",
    "        accuracy.loc[w,:] = [w, model_accuracy[1]]\n",
    "        \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2236 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2568 - accuracy: 0.8750\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3390 - accuracy: 0.8750\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.4822 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.1989 - accuracy: 0.6154\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.8511 - accuracy: 0.5333\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.8597 - accuracy: 0.5714\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1798 - accuracy: 0.6000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.7280 - accuracy: 0.2667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>Tensorflow Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   week Tensorflow Accuracy\n",
       "3     3                 1.0\n",
       "4     4               0.875\n",
       "5     5               0.875\n",
       "6     6                 0.5\n",
       "7     7            0.615385\n",
       "8     8            0.533333\n",
       "9     9            0.571429\n",
       "11   11                 0.6\n",
       "12   12            0.266667"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_Tensorflow(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow play area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_week = 7\n",
    "comp_games_df = df[df['week'] < pred_week]\n",
    "pred_games_df = df[df['week'] == pred_week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = comp_games_df\n",
    "test_df = pred_games_df\n",
    "\n",
    "X_train = train_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "y_train = train_df[['result']] \n",
    "X_test = test_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "y_test = test_df[['result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12)                300       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 78        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 385\n",
      "Trainable params: 385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = X_train.shape[1]\n",
    "\n",
    "# Review the number of features\n",
    "number_input_features\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons = 1\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1 =  (number_input_features + 1) // 2\n",
    "\n",
    "# Review the number hidden nodes in the first layer\n",
    "hidden_nodes_layer1\n",
    "\n",
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2 = (hidden_nodes_layer1 + 1) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "hidden_nodes_layer2\n",
    "\n",
    "# Create the Sequential model instance\n",
    "nn = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer1, activation='relu', input_dim=number_input_features))\n",
    "# Add the second hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer2, activation='relu'))\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn.add(Dense(units=number_output_neurons, activation='sigmoid'))\n",
    "# Display the Sequential model summary\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compile and fit the model using the `binary_crossentropy` loss function, the `adam` optimizer, and the `accuracy` evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model = nn.fit(X_train, y_train, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 2.1989 - accuracy: 0.6154 - 35ms/epoch - 35ms/step\n",
      "Loss: 2.198853015899658, Accuracy: 0.6153846383094788\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6153846383094788"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
