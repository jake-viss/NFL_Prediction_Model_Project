{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably going to have to install this if you haven't already\n",
    "#from sportsreference.nfl.boxscore import Boxscores, Boxscore\n",
    "from sportsipy.nfl.boxscore import Boxscores, Boxscore\n",
    "\n",
    "# the usual imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hvplot.pandas\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The game_Data function is extracting game statistics for each game. It loops through each game and each week grabbing the statistics referenced.\n",
    "# This function creates the foundation for our final dataset.\n",
    "def game_data(game_df,game_stats):\n",
    "    try:\n",
    "        # Creates a dataframe for the away_team and the home_team. Sets column names to be exact matches between the two.\n",
    "        away_team_df = game_df[['away_name', 'away_abbr', 'away_score']].rename(columns = {'away_name': 'team_name', 'away_abbr': 'team_abbr', 'away_score': 'score'})\n",
    "        home_team_df = game_df[['home_name','home_abbr', 'home_score']].rename(columns = {'home_name': 'team_name', 'home_abbr': 'team_abbr', 'home_score': 'score'})\n",
    "        try:\n",
    "            if game_df.loc[0,'away_score'] > game_df.loc[0,'home_score']:\n",
    "                away_team_df = pd.merge(away_team_df, pd.DataFrame({'game_won' : [1], 'game_lost' : [0]}),left_index = True, right_index = True)\n",
    "                home_team_df = pd.merge(home_team_df, pd.DataFrame({'game_won' : [0], 'game_lost' : [1]}),left_index = True, right_index = True)\n",
    "            elif game_df.loc[0,'away_score'] < game_df.loc[0,'home_score']:\n",
    "                away_team_df = pd.merge(away_team_df, pd.DataFrame({'game_won' : [0], 'game_lost' : [1]}),left_index = True, right_index = True)\n",
    "                home_team_df = pd.merge(home_team_df, pd.DataFrame({'game_won' : [1], 'game_lost' : [0]}),left_index = True, right_index = True)\n",
    "            else: \n",
    "                away_team_df = pd.merge(away_team_df, pd.DataFrame({'game_won' : [0], 'game_lost' : [0]}),left_index = True, right_index = True)\n",
    "                home_team_df = pd.merge(home_team_df, pd.DataFrame({'game_won' : [0], 'game_lost' : [0]}),left_index = True, right_index = True)\n",
    "        except TypeError:\n",
    "                away_team_df = pd.merge(away_team_df, pd.DataFrame({'game_won' : [np.nan], 'game_lost' : [np.nan]}),left_index = True, right_index = True)\n",
    "                home_team_df = pd.merge(home_team_df, pd.DataFrame({'game_won' : [np.nan], 'game_lost' : [np.nan]}),left_index = True, right_index = True)        \n",
    "\n",
    "        # Creating the away_team & home_team stats dataframe. Grabbing the selected stats and then renaming them to match home == away dataframe column names.\n",
    "        away_stats_df = game_stats.dataframe[['away_first_downs', 'away_fourth_down_attempts',\n",
    "               'away_fourth_down_conversions', 'away_fumbles', 'away_fumbles_lost',\n",
    "               'away_interceptions', 'away_net_pass_yards', 'away_pass_attempts',\n",
    "               'away_pass_completions', 'away_pass_touchdowns', 'away_pass_yards',\n",
    "               'away_penalties', 'away_points', 'away_rush_attempts',\n",
    "               'away_rush_touchdowns', 'away_rush_yards', 'away_third_down_attempts',\n",
    "               'away_third_down_conversions', 'away_time_of_possession',\n",
    "               'away_times_sacked', 'away_total_yards', 'away_turnovers',\n",
    "               'away_yards_from_penalties', 'away_yards_lost_from_sacks']].reset_index().drop(columns ='index').rename(columns = {\n",
    "               'away_first_downs': 'first_downs', 'away_fourth_down_attempts':'fourth_down_attempts',\n",
    "               'away_fourth_down_conversions':'fourth_down_conversions' , 'away_fumbles': 'fumbles', 'away_fumbles_lost': 'fumbles_lost',\n",
    "               'away_interceptions': 'interceptions', 'away_net_pass_yards':'net_pass_yards' , 'away_pass_attempts': 'pass_attempts',\n",
    "               'away_pass_completions':'pass_completions' , 'away_pass_touchdowns': 'pass_touchdowns', 'away_pass_yards': 'pass_yards',\n",
    "               'away_penalties': 'penalties', 'away_points': 'points', 'away_rush_attempts': 'rush_attempts',\n",
    "               'away_rush_touchdowns': 'rush_touchdowns', 'away_rush_yards': 'rush_yards', 'away_third_down_attempts': 'third_down_attempts',\n",
    "               'away_third_down_conversions': 'third_down_conversions', 'away_time_of_possession': 'time_of_possession',\n",
    "               'away_times_sacked': 'times_sacked', 'away_total_yards': 'total_yards', 'away_turnovers': 'turnovers',\n",
    "               'away_yards_from_penalties':'yards_from_penalties', 'away_yards_lost_from_sacks': 'yards_lost_from_sacks'})\n",
    "\n",
    "        home_stats_df = game_stats.dataframe[['home_first_downs', 'home_fourth_down_attempts',\n",
    "               'home_fourth_down_conversions', 'home_fumbles', 'home_fumbles_lost',\n",
    "               'home_interceptions', 'home_net_pass_yards', 'home_pass_attempts',\n",
    "               'home_pass_completions', 'home_pass_touchdowns', 'home_pass_yards',\n",
    "               'home_penalties', 'home_points', 'home_rush_attempts',\n",
    "               'home_rush_touchdowns', 'home_rush_yards', 'home_third_down_attempts',\n",
    "               'home_third_down_conversions', 'home_time_of_possession',\n",
    "               'home_times_sacked', 'home_total_yards', 'home_turnovers',\n",
    "               'home_yards_from_penalties', 'home_yards_lost_from_sacks']].reset_index().drop(columns = 'index').rename(columns = {\n",
    "               'home_first_downs': 'first_downs', 'home_fourth_down_attempts':'fourth_down_attempts',\n",
    "               'home_fourth_down_conversions':'fourth_down_conversions' , 'home_fumbles': 'fumbles', 'home_fumbles_lost': 'fumbles_lost',\n",
    "               'home_interceptions': 'interceptions', 'home_net_pass_yards':'net_pass_yards' , 'home_pass_attempts': 'pass_attempts',\n",
    "               'home_pass_completions':'pass_completions' , 'home_pass_touchdowns': 'pass_touchdowns', 'home_pass_yards': 'pass_yards',\n",
    "               'home_penalties': 'penalties', 'home_points': 'points', 'home_rush_attempts': 'rush_attempts',\n",
    "               'home_rush_touchdowns': 'rush_touchdowns', 'home_rush_yards': 'rush_yards', 'home_third_down_attempts': 'third_down_attempts',\n",
    "               'home_third_down_conversions': 'third_down_conversions', 'home_time_of_possession': 'time_of_possession',\n",
    "               'home_times_sacked': 'times_sacked', 'home_total_yards': 'total_yards', 'home_turnovers': 'turnovers',\n",
    "               'home_yards_from_penalties':'yards_from_penalties', 'home_yards_lost_from_sacks': 'yards_lost_from_sacks'})\n",
    "        \n",
    "        # Merge the team_df & stats_df for both home & away teams. Set the left_index & right_index to True so that both dataframes merge on the same indices. \n",
    "        away_team_df = pd.merge(away_team_df, away_stats_df,left_index = True, right_index = True)\n",
    "        home_team_df = pd.merge(home_team_df, home_stats_df,left_index = True, right_index = True)\n",
    "        try:\n",
    "            # Converting time_of_possession from MM:SS format into seconds(int). \n",
    "            away_team_df['time_of_possession'] = (int(away_team_df['time_of_possession'].loc[0][0:2]) * 60) + int(away_team_df['time_of_possession'].loc[0][3:5])\n",
    "            home_team_df['time_of_possession'] = (int(home_team_df['time_of_possession'].loc[0][0:2]) * 60) + int(home_team_df['time_of_possession'].loc[0][3:5])\n",
    "        except TypeError:\n",
    "            away_team_df['time_of_possession'] = np.nan\n",
    "            home_team_df['time_of_possession'] = np.nan\n",
    "    except TypeError:\n",
    "        away_team_df = pd.DataFrame()\n",
    "        home_team_df = pd.DataFrame()\n",
    "    return away_team_df, home_team_df\n",
    "\n",
    "def game_data_up_to_week(weeks,year):\n",
    "    weeks_games_df = pd.DataFrame()\n",
    "    for w in range(len(weeks)):\n",
    "        date_string = str(weeks[w]) + '-' + str(year)\n",
    "        week_scores = Boxscores(weeks[w],year)\n",
    "        week_games_df = pd.DataFrame()\n",
    "        for g in range(len(week_scores.games[date_string])):\n",
    "            game_str = week_scores.games[date_string][g]['boxscore']\n",
    "            game_stats = Boxscore(game_str)\n",
    "            game_df = pd.DataFrame(week_scores.games[date_string][g], index = [0])\n",
    "            away_team_df, home_team_df = game_data(game_df,game_stats)\n",
    "            away_team_df['week'] = weeks[w]\n",
    "            home_team_df['week'] = weeks[w]\n",
    "            week_games_df = pd.concat([week_games_df,away_team_df])\n",
    "            week_games_df = pd.concat([week_games_df,home_team_df])\n",
    "        weeks_games_df = pd.concat([weeks_games_df,week_games_df])\n",
    "    return weeks_games_df\n",
    "\n",
    "def get_schedule(year):\n",
    "    weeks = list(range(1,18))\n",
    "    schedule_df = pd.DataFrame()\n",
    "    for w in range(len(weeks)):\n",
    "        date_string = str(weeks[w]) + '-' + str(year)\n",
    "        week_scores = Boxscores(weeks[w],year)\n",
    "        week_games_df = pd.DataFrame()\n",
    "        for g in range(len(week_scores.games[date_string])):\n",
    "            game = pd.DataFrame(week_scores.games[date_string][g], index = [0])[['away_name', 'away_abbr','home_name', 'home_abbr','winning_name', 'winning_abbr' ]]\n",
    "            game['week'] = weeks[w]\n",
    "            week_games_df = pd.concat([week_games_df,game])\n",
    "        schedule_df = pd.concat([schedule_df, week_games_df]).reset_index().drop(columns = 'index') \n",
    "    return schedule_df \n",
    "\n",
    "def agg_weekly_data(schedule_df,weeks_games_df,current_week,weeks):\n",
    "    schedule_df = schedule_df[schedule_df.week < current_week]\n",
    "    agg_games_df = pd.DataFrame()\n",
    "    for w in range(1,len(weeks)):\n",
    "        games_df = schedule_df[schedule_df.week == weeks[w]]\n",
    "        agg_weekly_df = weeks_games_df[weeks_games_df.week < weeks[w]].drop(columns = ['score','week','game_won', 'game_lost']).groupby(by=[\"team_name\", \"team_abbr\"]).mean().reset_index()\n",
    "        win_loss_df = weeks_games_df[weeks_games_df.week < weeks[w]][[\"team_name\", \"team_abbr\",'game_won', 'game_lost']].groupby(by=[\"team_name\", \"team_abbr\"]).sum().reset_index()\n",
    "        win_loss_df['win_perc'] = win_loss_df['game_won'] / (win_loss_df['game_won'] + win_loss_df['game_lost'])\n",
    "        win_loss_df = win_loss_df.drop(columns = ['game_won', 'game_lost'])\n",
    "\n",
    "        try:\n",
    "            agg_weekly_df['fourth_down_perc'] = agg_weekly_df['fourth_down_conversions'] / agg_weekly_df['fourth_down_attempts']  \n",
    "        except ZeroDivisionError:\n",
    "            agg_weekly_df['fourth_down_perc'] = 0 \n",
    "        agg_weekly_df['fourth_down_perc'] = agg_weekly_df['fourth_down_perc'].fillna(0)\n",
    "\n",
    "        try:\n",
    "            agg_weekly_df['third_down_perc'] = agg_weekly_df['third_down_conversions'] / agg_weekly_df['third_down_attempts']  \n",
    "        except ZeroDivisionError:\n",
    "            agg_weekly_df['third_down_perc'] = 0\n",
    "        agg_weekly_df['third_down_perc'] = agg_weekly_df['third_down_perc'].fillna(0)  \n",
    "\n",
    "        agg_weekly_df = agg_weekly_df.drop(columns = ['fourth_down_attempts', 'fourth_down_conversions', 'third_down_attempts', 'third_down_conversions'])\n",
    "        agg_weekly_df = pd.merge(win_loss_df,agg_weekly_df,left_on = ['team_name', 'team_abbr'], right_on = ['team_name', 'team_abbr'])\n",
    "\n",
    "        away_df = pd.merge(games_df,agg_weekly_df,how = 'inner', left_on = ['away_name', 'away_abbr'], right_on = ['team_name', 'team_abbr']).drop(columns = ['team_name', 'team_abbr']).rename(columns = {\n",
    "                'win_perc': 'away_win_perc',\n",
    "               'first_downs': 'away_first_downs', 'fumbles': 'away_fumbles', 'fumbles_lost':'away_fumbles_lost', 'interceptions':'away_interceptions',\n",
    "               'net_pass_yards': 'away_net_pass_yards', 'pass_attempts':'away_pass_attempts', 'pass_completions':'away_pass_completions',\n",
    "               'pass_touchdowns':'away_pass_touchdowns', 'pass_yards':'away_pass_yards', 'penalties':'away_penalties', 'points':'away_points', 'rush_attempts':'away_rush_attempts',\n",
    "               'rush_touchdowns':'away_rush_touchdowns', 'rush_yards':'away_rush_yards', 'time_of_possession':'away_time_of_possession', 'times_sacked':'away_times_sacked',\n",
    "               'total_yards':'away_total_yards', 'turnovers':'away_turnovers', 'yards_from_penalties':'away_yards_from_penalties',\n",
    "               'yards_lost_from_sacks': 'away_yards_lost_from_sacks', 'fourth_down_perc':'away_fourth_down_perc', 'third_down_perc':'away_third_down_perc'})\n",
    "\n",
    "        home_df = pd.merge(games_df,agg_weekly_df,how = 'inner', left_on = ['home_name', 'home_abbr'], right_on = ['team_name', 'team_abbr']).drop(columns = ['team_name', 'team_abbr']).rename(columns = {\n",
    "                'win_perc': 'home_win_perc',\n",
    "               'first_downs': 'home_first_downs', 'fumbles': 'home_fumbles', 'fumbles_lost':'home_fumbles_lost', 'interceptions':'home_interceptions',\n",
    "               'net_pass_yards': 'home_net_pass_yards', 'pass_attempts':'home_pass_attempts', 'pass_completions':'home_pass_completions',\n",
    "               'pass_touchdowns':'home_pass_touchdowns', 'pass_yards':'home_pass_yards', 'penalties':'home_penalties', 'points':'home_points', 'rush_attempts':'home_rush_attempts',\n",
    "               'rush_touchdowns':'home_rush_touchdowns', 'rush_yards':'home_rush_yards', 'time_of_possession':'home_time_of_possession', 'times_sacked':'home_times_sacked',\n",
    "               'total_yards':'home_total_yards', 'turnovers':'home_turnovers', 'yards_from_penalties':'home_yards_from_penalties',\n",
    "               'yards_lost_from_sacks': 'home_yards_lost_from_sacks', 'fourth_down_perc':'home_fourth_down_perc', 'third_down_perc':'home_third_down_perc'})\n",
    "\n",
    "        agg_weekly_df = pd.merge(away_df,home_df,left_on = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'winning_name',\n",
    "               'winning_abbr', 'week'], right_on = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'winning_name',\n",
    "               'winning_abbr', 'week'])\n",
    "\n",
    "        agg_weekly_df['win_perc_dif'] = agg_weekly_df['away_win_perc'] - agg_weekly_df['home_win_perc']\n",
    "        agg_weekly_df['first_downs_dif'] = agg_weekly_df['away_first_downs'] - agg_weekly_df['home_first_downs']\n",
    "        agg_weekly_df['fumbles_dif'] = agg_weekly_df['away_fumbles'] - agg_weekly_df['home_fumbles']\n",
    "        agg_weekly_df['interceptions_dif'] = agg_weekly_df['away_interceptions'] - agg_weekly_df['home_interceptions']\n",
    "        agg_weekly_df['net_pass_yards_dif'] = agg_weekly_df['away_net_pass_yards'] - agg_weekly_df['home_net_pass_yards']\n",
    "        agg_weekly_df['pass_attempts_dif'] = agg_weekly_df['away_pass_attempts'] - agg_weekly_df['home_pass_attempts']\n",
    "        agg_weekly_df['pass_completions_dif'] = agg_weekly_df['away_pass_completions'] - agg_weekly_df['home_pass_completions']\n",
    "        agg_weekly_df['pass_touchdowns_dif'] = agg_weekly_df['away_pass_touchdowns'] - agg_weekly_df['home_pass_touchdowns']\n",
    "        agg_weekly_df['pass_yards_dif'] = agg_weekly_df['away_pass_yards'] - agg_weekly_df['home_pass_yards']\n",
    "        agg_weekly_df['penalties_dif'] = agg_weekly_df['away_penalties'] - agg_weekly_df['home_penalties']\n",
    "        agg_weekly_df['points_dif'] = agg_weekly_df['away_points'] - agg_weekly_df['home_points']\n",
    "        agg_weekly_df['rush_attempts_dif'] = agg_weekly_df['away_rush_attempts'] - agg_weekly_df['home_rush_attempts']\n",
    "        agg_weekly_df['rush_touchdowns_dif'] = agg_weekly_df['away_rush_touchdowns'] - agg_weekly_df['home_rush_touchdowns']\n",
    "        agg_weekly_df['rush_yards_dif'] = agg_weekly_df['away_rush_yards'] - agg_weekly_df['home_rush_yards']\n",
    "        agg_weekly_df['time_of_possession_dif'] = agg_weekly_df['away_time_of_possession'] - agg_weekly_df['home_time_of_possession']\n",
    "        agg_weekly_df['times_sacked_dif'] = agg_weekly_df['away_times_sacked'] - agg_weekly_df['home_times_sacked']\n",
    "        agg_weekly_df['total_yards_dif'] = agg_weekly_df['away_total_yards'] - agg_weekly_df['home_total_yards']\n",
    "        agg_weekly_df['turnovers_dif'] = agg_weekly_df['away_turnovers'] - agg_weekly_df['home_turnovers']\n",
    "        agg_weekly_df['yards_from_penalties_dif'] = agg_weekly_df['away_yards_from_penalties'] - agg_weekly_df['home_yards_from_penalties']\n",
    "        agg_weekly_df['yards_lost_from_sacks_dif'] = agg_weekly_df['away_yards_lost_from_sacks'] - agg_weekly_df['home_yards_lost_from_sacks']\n",
    "        agg_weekly_df['fourth_down_perc_dif'] = agg_weekly_df['away_fourth_down_perc'] - agg_weekly_df['home_fourth_down_perc']\n",
    "        agg_weekly_df['third_down_perc_dif'] = agg_weekly_df['away_third_down_perc'] - agg_weekly_df['home_third_down_perc']\n",
    "\n",
    "        agg_weekly_df = agg_weekly_df.drop(columns = ['away_win_perc',\n",
    "               'away_first_downs', 'away_fumbles', 'away_fumbles_lost', 'away_interceptions',\n",
    "               'away_net_pass_yards', 'away_pass_attempts','away_pass_completions',\n",
    "               'away_pass_touchdowns', 'away_pass_yards', 'away_penalties', 'away_points', 'away_rush_attempts',\n",
    "               'away_rush_touchdowns', 'away_rush_yards', 'away_time_of_possession', 'away_times_sacked',\n",
    "               'away_total_yards', 'away_turnovers', 'away_yards_from_penalties',\n",
    "               'away_yards_lost_from_sacks','away_fourth_down_perc', 'away_third_down_perc','home_win_perc',\n",
    "               'home_first_downs', 'home_fumbles', 'home_fumbles_lost', 'home_interceptions',\n",
    "               'home_net_pass_yards', 'home_pass_attempts','home_pass_completions',\n",
    "               'home_pass_touchdowns', 'home_pass_yards', 'home_penalties', 'home_points', 'home_rush_attempts',\n",
    "               'home_rush_touchdowns', 'home_rush_yards', 'home_time_of_possession', 'home_times_sacked',\n",
    "               'home_total_yards', 'home_turnovers', 'home_yards_from_penalties',\n",
    "               'home_yards_lost_from_sacks','home_fourth_down_perc', 'home_third_down_perc'])\n",
    "            \n",
    "        if (weeks[w] == current_week and weeks[w] > 3 and agg_weekly_df['winning_name'].isnull().values.any()):\n",
    "            agg_weekly_df['result'] = np.nan\n",
    "            print(f\"Week {weeks[w]} games have not finished yet.\")\n",
    "        \n",
    "#         if (agg_weekly_df['winning_name'].isnull().values.any() and weeks[w] > 3):\n",
    "#             agg_weekly_df['result'] = np.nan\n",
    "#             print(f\"Week {weeks[w]} games have not finished yet.\")\n",
    "        else:\n",
    "            agg_weekly_df['result'] = agg_weekly_df['winning_name'] == agg_weekly_df['away_name']\n",
    "            agg_weekly_df['result'] = agg_weekly_df['result'].astype('float')\n",
    "        agg_weekly_df = agg_weekly_df.drop(columns = ['winning_name', 'winning_abbr'])\n",
    "        agg_games_df = pd.concat([agg_games_df, agg_weekly_df])\n",
    "    agg_games_df = agg_games_df.reset_index().drop(columns = 'index')\n",
    "    # What is .drop(index = 20) doing?\n",
    "    agg_games_df = agg_games_df.drop(index = 20, axis=0)\n",
    "    return agg_games_df\n",
    "\n",
    "def get_elo():\n",
    "    elo_df = pd.read_csv('https://projects.fivethirtyeight.com/nfl-api/nfl_elo_latest.csv')\n",
    "    elo_df = elo_df.drop(columns = ['season','neutral' ,'playoff', 'elo_prob1', 'elo_prob2', 'elo1_post', 'elo2_post',\n",
    "           'qbelo1_pre', 'qbelo2_pre', 'qb1', 'qb2', 'qb1_adj', 'qb2_adj', 'qbelo_prob1', 'qbelo_prob2',\n",
    "           'qb1_game_value', 'qb2_game_value', 'qb1_value_post', 'qb2_value_post',\n",
    "           'qbelo1_post', 'qbelo2_post', 'score1', 'score2', 'quality', 'importance', 'total_rating'])\n",
    "    elo_df.date = pd.to_datetime(elo_df.date)\n",
    "    elo_df = elo_df[elo_df.date < '01-05-2022']\n",
    "\n",
    "    elo_df['team1'] = elo_df['team1'].replace(['KC', 'JAX', 'CAR', 'BAL', 'BUF', 'MIN', 'DET', 'ATL', 'NE', 'WSH',\n",
    "           'CIN', 'NO', 'SF', 'LAR', 'NYG', 'DEN', 'CLE', 'IND', 'TEN', 'NYJ',\n",
    "           'TB', 'MIA', 'PIT', 'PHI', 'GB', 'CHI', 'DAL', 'ARI', 'LAC', 'HOU',\n",
    "           'SEA', 'OAK'],\n",
    "            ['kan','jax','car', 'rav', 'buf', 'min', 'det', 'atl', 'nwe', 'was', \n",
    "            'cin', 'nor', 'sfo', 'ram', 'nyg', 'den', 'cle', 'clt', 'oti', 'nyj', \n",
    "             'tam','mia', 'pit', 'phi', 'gnb', 'chi', 'dal', 'crd', 'sdg', 'htx', 'sea', 'rai' ])\n",
    "    elo_df['team2'] = elo_df['team2'].replace(['KC', 'JAX', 'CAR', 'BAL', 'BUF', 'MIN', 'DET', 'ATL', 'NE', 'WSH',\n",
    "           'CIN', 'NO', 'SF', 'LAR', 'NYG', 'DEN', 'CLE', 'IND', 'TEN', 'NYJ',\n",
    "           'TB', 'MIA', 'PIT', 'PHI', 'GB', 'CHI', 'DAL', 'ARI', 'LAC', 'HOU',\n",
    "           'SEA', 'OAK'],\n",
    "            ['kan','jax','car', 'rav', 'buf', 'min', 'det', 'atl', 'nwe', 'was', \n",
    "            'cin', 'nor', 'sfo', 'ram', 'nyg', 'den', 'cle', 'clt', 'oti', 'nyj', \n",
    "             'tam','mia', 'pit', 'phi', 'gnb', 'chi', 'dal', 'crd', 'sdg', 'htx', 'sea', 'rai' ])\n",
    "    return elo_df\n",
    "\n",
    "def merge_rankings(agg_games_df,elo_df):\n",
    "    agg_games_df = pd.merge(agg_games_df, elo_df, how = 'inner', left_on = ['home_abbr', 'away_abbr'], right_on = ['team1', 'team2']).drop(columns = ['date','team1', 'team2'])\n",
    "    agg_games_df['elo_dif'] = agg_games_df['elo2_pre'] - agg_games_df['elo1_pre']\n",
    "    agg_games_df['qb_dif'] = agg_games_df['qb2_value_pre'] - agg_games_df['qb1_value_pre']\n",
    "    agg_games_df = agg_games_df.drop(columns = ['elo1_pre', 'elo2_pre', 'qb1_value_pre', 'qb2_value_pre'])\n",
    "    return agg_games_df\n",
    "\n",
    "def prep_test_train(current_week,weeks,year):\n",
    "    current_week = current_week + 1\n",
    "    schedule_df  = get_schedule(year)\n",
    "    weeks_games_df = game_data_up_to_week(weeks,year)\n",
    "    agg_games_df = agg_weekly_data(schedule_df,weeks_games_df,current_week,weeks)\n",
    "    elo_df = get_elo()\n",
    "    agg_games_df = merge_rankings(agg_games_df, elo_df)\n",
    "    train_df = agg_games_df[agg_games_df.result.notna()]\n",
    "    current_week = current_week - 1\n",
    "    test_df = agg_games_df[agg_games_df.week == current_week]\n",
    "    return test_df, train_df\n",
    "\n",
    "def display(y_pred,X_test):\n",
    "    for g in range(len(y_pred)):\n",
    "        #win_prob = np.round(y_pred[g],2)\n",
    "        win_prob = int(y_pred[g] * 100)\n",
    "        away_team = X_test.reset_index().drop(columns = 'index').loc[g,'away_name']\n",
    "        home_team = X_test.reset_index().drop(columns = 'index').loc[g,'home_name']\n",
    "        print(f'The {away_team} have a probability of {win_prob}% of beating the {home_team}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the data for 2021, up to the current week 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this step takes about five minutes to run \n",
    "current_week = 14\n",
    "weeks = list(range(1,current_week + 1))\n",
    "year = 2021\n",
    "\n",
    "pred_games_df, comp_games_df = prep_test_train(current_week,weeks,year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate that get picked up for last week\n",
    "pred_games_df = pred_games_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate that get picked up for last week\n",
    "df = pd.concat([comp_games_df, pred_games_df], axis=0).drop_duplicates()\n",
    "\n",
    "# Write out 2021 info, so I don't have to load it all the time\n",
    "df.to_csv(\"2021_week_2_through_14.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the full dataframe in and drop an unneeded column\n",
    "df = pd.read_csv('2021_week_2_through_14.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_week = 12\n",
    "comp_games_df = df[df['week'] < pred_week]\n",
    "pred_games_df = df[df['week'] == pred_week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = comp_games_df\n",
    "test_df = pred_games_df\n",
    "\n",
    "X_train = train_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "y_train = train_df[['result']] \n",
    "X_test = test_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "y_test = test_df[['result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Unscaled\n",
      "\n",
      "The Chicago Bears have a probability of 57% of beating the Detroit Lions.\n",
      "The Las Vegas Raiders have a probability of 30% of beating the Dallas Cowboys.\n",
      "The Buffalo Bills have a probability of 80% of beating the New Orleans Saints.\n",
      "The Philadelphia Eagles have a probability of 55% of beating the New York Giants.\n",
      "The Tennessee Titans have a probability of 50% of beating the New England Patriots.\n",
      "The Carolina Panthers have a probability of 44% of beating the Miami Dolphins.\n",
      "The Atlanta Falcons have a probability of 46% of beating the Jacksonville Jaguars.\n",
      "The New York Jets have a probability of 59% of beating the Houston Texans.\n",
      "The Tampa Bay Buccaneers have a probability of 43% of beating the Indianapolis Colts.\n",
      "The Pittsburgh Steelers have a probability of 29% of beating the Cincinnati Bengals.\n",
      "The Los Angeles Chargers have a probability of 41% of beating the Denver Broncos.\n",
      "The Minnesota Vikings have a probability of 49% of beating the San Francisco 49ers.\n",
      "The Los Angeles Rams have a probability of 41% of beating the Green Bay Packers.\n",
      "The Cleveland Browns have a probability of 34% of beating the Baltimore Ravens.\n",
      "The Seattle Seahawks have a probability of 33% of beating the Washington Football Team.\n",
      "\n",
      "Logistic Regression - Scaled\n",
      "\n",
      "The Chicago Bears have a probability of 61% of beating the Detroit Lions.\n",
      "The Las Vegas Raiders have a probability of 29% of beating the Dallas Cowboys.\n",
      "The Buffalo Bills have a probability of 71% of beating the New Orleans Saints.\n",
      "The Philadelphia Eagles have a probability of 60% of beating the New York Giants.\n",
      "The Tennessee Titans have a probability of 51% of beating the New England Patriots.\n",
      "The Carolina Panthers have a probability of 45% of beating the Miami Dolphins.\n",
      "The Atlanta Falcons have a probability of 45% of beating the Jacksonville Jaguars.\n",
      "The New York Jets have a probability of 57% of beating the Houston Texans.\n",
      "The Tampa Bay Buccaneers have a probability of 47% of beating the Indianapolis Colts.\n",
      "The Pittsburgh Steelers have a probability of 39% of beating the Cincinnati Bengals.\n",
      "The Los Angeles Chargers have a probability of 56% of beating the Denver Broncos.\n",
      "The Minnesota Vikings have a probability of 49% of beating the San Francisco 49ers.\n",
      "The Los Angeles Rams have a probability of 43% of beating the Green Bay Packers.\n",
      "The Cleveland Browns have a probability of 31% of beating the Baltimore Ravens.\n",
      "The Seattle Seahawks have a probability of 33% of beating the Washington Football Team.\n"
     ]
    }
   ],
   "source": [
    "# Logistic Model\n",
    "clf_unscaled = LogisticRegression(penalty='l1', dual=False, tol=0.001, C=1.0, fit_intercept=True, \n",
    "                   intercept_scaling=1, class_weight='balanced', random_state=None, \n",
    "                   solver='liblinear', max_iter=1000, multi_class='ovr', verbose=0)\n",
    "\n",
    "clf_scaled = LogisticRegression(penalty='l1', dual=False, tol=0.001, C=1.0, fit_intercept=True, \n",
    "                   intercept_scaling=1, class_weight='balanced', random_state=None, \n",
    "                   solver='liblinear', max_iter=1000, multi_class='ovr', verbose=0)\n",
    "\n",
    "clf_unscaled.fit(X_train, np.ravel(y_train.values))\n",
    "clf_scaled.fit(X_train_scaled, np.ravel(y_train.values))\n",
    "\n",
    "y_pred_unscaled = clf_unscaled.predict_proba(X_test)\n",
    "y_pred_scaled = clf_scaled.predict_proba(X_test_scaled)\n",
    "\n",
    "y_pred_unscaled = y_pred_unscaled[:,1]\n",
    "y_pred_scaled = y_pred_scaled[:,1]\n",
    "\n",
    "print(\"Logistic Regression - Unscaled\\n\")\n",
    "display(y_pred_unscaled,test_df)\n",
    "\n",
    "print(\"\\nLogistic Regression - Scaled\\n\")\n",
    "display(y_pred_scaled,test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Logistic Regression Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a for loop to the get accuracy for each week\n",
    "def accuracy_score_log_reg(df):\n",
    "    \n",
    "    accuracy = pd.DataFrame(\n",
    "        columns=[\n",
    "            'week', \n",
    "            'Logistic Regression Accuracy - Unscaled', \n",
    "            'Logistic Regeression Accuracy - Scaled',\n",
    "            'Log Reg - Unscaled - drop 40% to 60%',\n",
    "            'Log Reg - Scaled - drop 40% to 60%'\n",
    "        ])\n",
    "    \n",
    "    for w in df['week'].unique()[1:-1]:\n",
    "        train_df = df[df['week'] < w]\n",
    "        test_df = df[df['week'] == w]\n",
    "        \n",
    "        X_train = train_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "        y_train = train_df[['result']] \n",
    "        X_test = test_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "        y_test = test_df[['result']]\n",
    "        \n",
    "        # Create a StandardScaler instance\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Fit the scaler to the features training dataset\n",
    "        X_scaler = scaler.fit(X_train)\n",
    "\n",
    "        # Fit the scaler to the features training dataset\n",
    "        X_train_scaled = X_scaler.transform(X_train)\n",
    "        X_test_scaled = X_scaler.transform(X_test)\n",
    "        \n",
    "        clf_unscaled = LogisticRegression(penalty='l1', dual=False, tol=0.001, C=1.0, fit_intercept=True, \n",
    "                           intercept_scaling=1, class_weight='balanced', random_state=None, \n",
    "                           solver='liblinear', max_iter=1000, multi_class='ovr', verbose=0)\n",
    "\n",
    "        clf_scaled = LogisticRegression(penalty='l1', dual=False, tol=0.001, C=1.0, fit_intercept=True, \n",
    "                           intercept_scaling=1, class_weight='balanced', random_state=None, \n",
    "                           solver='liblinear', max_iter=1000, multi_class='ovr', verbose=0)\n",
    "\n",
    "        clf_unscaled.fit(X_train, np.ravel(y_train.values))\n",
    "        clf_scaled.fit(X_train_scaled, np.ravel(y_train.values))\n",
    "        \n",
    "        y_pred_unscaled = clf_unscaled.predict_proba(X_test)\n",
    "        y_pred_scaled = clf_scaled.predict_proba(X_test_scaled)\n",
    "        \n",
    "        y_pred_unscaled = y_pred_unscaled[:,1]\n",
    "        y_pred_scaled = y_pred_scaled[:,1]\n",
    "        \n",
    "        accuracy_score_unscaled = accuracy_score(y_test, np.round(y_pred_unscaled))\n",
    "        accuracy_score_scaled = accuracy_score(y_test, np.round(y_pred_scaled))\n",
    "        accuracy_score_unscaled_drop_40_60 = accuracy_score(y_test[(y_pred_unscaled < .4) | (y_pred_unscaled > .6)], np.round(y_pred_unscaled[(y_pred_unscaled < .4) | (y_pred_unscaled > .6)]))\n",
    "        accuracy_score_scaled_drop_40_60 = accuracy_score(y_test[(y_pred_scaled < .4) | (y_pred_scaled > .6)],np.round(y_pred_scaled[(y_pred_scaled < .4) | (y_pred_scaled > .6)]))\n",
    "        \n",
    "        accuracy.loc[w,:] = [w, accuracy_score_unscaled, accuracy_score_scaled, accuracy_score_unscaled_drop_40_60, accuracy_score_scaled_drop_40_60]\n",
    "        \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>Logistic Regression Accuracy - Unscaled</th>\n",
       "      <th>Logistic Regeression Accuracy - Scaled</th>\n",
       "      <th>Log Reg - Unscaled - drop 40% to 60%</th>\n",
       "      <th>Log Reg - Scaled - drop 40% to 60%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   week Logistic Regression Accuracy - Unscaled  \\\n",
       "3     3                                     0.6   \n",
       "4     4                                  0.6875   \n",
       "5     5                                     0.5   \n",
       "6     6                                0.857143   \n",
       "7     7                                0.769231   \n",
       "8     8                                0.466667   \n",
       "9     9                                0.642857   \n",
       "10   10                                     0.5   \n",
       "11   11                                0.666667   \n",
       "12   12                                0.666667   \n",
       "13   13                                0.571429   \n",
       "\n",
       "   Logistic Regeression Accuracy - Scaled  \\\n",
       "3                                0.466667   \n",
       "4                                  0.6875   \n",
       "5                                   0.625   \n",
       "6                                0.857143   \n",
       "7                                0.769231   \n",
       "8                                0.466667   \n",
       "9                                0.714286   \n",
       "10                                    0.5   \n",
       "11                                    0.6   \n",
       "12                                    0.6   \n",
       "13                               0.571429   \n",
       "\n",
       "   Log Reg - Unscaled - drop 40% to 60% Log Reg - Scaled - drop 40% to 60%  \n",
       "3                              0.538462                           0.555556  \n",
       "4                              0.666667                           0.692308  \n",
       "5                              0.466667                           0.666667  \n",
       "6                              0.833333                           0.909091  \n",
       "7                                  0.75                                0.8  \n",
       "8                                   0.5                           0.466667  \n",
       "9                                   0.6                           0.636364  \n",
       "10                                  0.5                           0.538462  \n",
       "11                             0.777778                           0.777778  \n",
       "12                                  0.8                           0.714286  \n",
       "13                                  0.6                                0.6  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the accuracy function and output the df.\n",
    "logistic_regression_accuracy = accuracy_score_log_reg(df)\n",
    "logistic_regression_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week we want to predict\n",
    "pred_week = 14\n",
    "\n",
    "# To build a tensorflow model, we're going to use the three weeks prior to the week we are trying to predict.\n",
    "# tensorflow tends to overfit models on the data it has.\n",
    "comp_games_df = df[(df['week'] >= (pred_week - 3)) & (df['week'] < pred_week)]\n",
    "pred_games_df = df[df['week'] == pred_week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = comp_games_df\n",
    "test_df = pred_games_df\n",
    "\n",
    "X_train = train_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "y_train = train_df[['result']] \n",
    "X_test = test_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "y_test = test_df[['result']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and build the tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12)                300       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 78        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 21        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 403\n",
      "Trainable params: 403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = X_train.shape[1]\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons = 1\n",
    "\n",
    "# Define the number of hidden nodes for three layer\n",
    "hidden_nodes_layer1 =  (number_input_features + 1) // 2\n",
    "hidden_nodes_layer2 = (hidden_nodes_layer1 + 1) // 2\n",
    "hidden_nodes_layer3 = (hidden_nodes_layer2 + 1) // 2\n",
    "\n",
    "# Create the Sequential model instance\n",
    "nn = Sequential()\n",
    "\n",
    "# Add the hidden layers\n",
    "nn.add(Dense(units=hidden_nodes_layer1, activation='relu', input_dim=number_input_features))\n",
    "nn.add(Dense(units=hidden_nodes_layer2, activation='relu'))\n",
    "nn.add(Dense(units=hidden_nodes_layer3, activation='relu'))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn.add(Dense(units=number_output_neurons, activation='sigmoid'))\n",
    "\n",
    "# Display the Sequential model summary\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the Sequential model\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model = nn.fit(X_train, y_train, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's file path\n",
    "#file_path = \"Resources/model_nn_2021_2_12_not_scaled_relu_relu_sigmoid_loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']_1000 epochs.h5\"\n",
    "file_path = \"Resources/model_nn_2021_weeks_11_through_12.h5\"\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn.save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the tensorflowe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a previously built model.\n",
    "#file_path = \"Resources/model_nn_relu_relu_sigmoid_loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']_200 epochs.h5\"\n",
    "#file_path = \"Resources/model_nn_relu_relu_sigmoid_loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']_200 epochs.h5\"\n",
    "\n",
    "nn = tf.keras.models.load_model(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow - Unscaled \n",
      "\n",
      "The Pittsburgh Steelers have a probability of 92% of beating the Minnesota Vikings.\n",
      "The San Francisco 49ers have a probability of 81% of beating the Cincinnati Bengals.\n",
      "The Atlanta Falcons have a probability of 92% of beating the Carolina Panthers.\n",
      "The Dallas Cowboys have a probability of 1% of beating the Washington Football Team.\n",
      "The Jacksonville Jaguars have a probability of 1% of beating the Tennessee Titans.\n",
      "The New Orleans Saints have a probability of 95% of beating the New York Jets.\n",
      "The Las Vegas Raiders have a probability of 0% of beating the Kansas City Chiefs.\n",
      "The Seattle Seahawks have a probability of 0% of beating the Houston Texans.\n",
      "The Baltimore Ravens have a probability of 99% of beating the Cleveland Browns.\n",
      "The New York Giants have a probability of 83% of beating the Los Angeles Chargers.\n",
      "The Detroit Lions have a probability of 81% of beating the Denver Broncos.\n",
      "The Buffalo Bills have a probability of 0% of beating the Tampa Bay Buccaneers.\n",
      "The Chicago Bears have a probability of 7% of beating the Green Bay Packers.\n",
      "The Los Angeles Rams have a probability of 89% of beating the Arizona Cardinals.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data        \n",
    "nn_predictions_unscaled = nn.predict(X_test)       \n",
    "\n",
    "print(\"Tensorflow - Unscaled \\n\")\n",
    "display(nn_predictions_unscaled.squeeze(),test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a for loop to the get accuracy for each week\n",
    "def accuracy_score_tensorflow(df):\n",
    "    \n",
    "    accuracy = pd.DataFrame(columns=['week', 'Tensorflow Accuracy - Unscaled'])\n",
    "    \n",
    "    for w in df['week'].unique()[1:-1]:\n",
    "        train_df = df[df['week'] < w]\n",
    "        test_df = df[df['week'] == w]\n",
    "        \n",
    "        X_train = train_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "        y_train = train_df[['result']] \n",
    "        X_test = test_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "        y_test = test_df[['result']]\n",
    "        \n",
    "        # Evaluate the model loss and accuracy metrics using the evaluate method and the test data        \n",
    "        model_accuracy_unscaled = nn.evaluate(X_test, y_test, verbose=0)       \n",
    "    \n",
    "        # Assign the accuracy to \n",
    "        accuracy.loc[w,:] = [w, model_accuracy_unscaled[1]]\n",
    "        \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>Tensorflow Accuracy - Unscaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   week Tensorflow Accuracy - Unscaled\n",
       "3     3                            0.4\n",
       "4     4                         0.4375\n",
       "5     5                          0.375\n",
       "6     6                       0.571429\n",
       "7     7                       0.692308\n",
       "8     8                       0.666667\n",
       "9     9                       0.285714\n",
       "10   10                            0.5\n",
       "11   11                            1.0\n",
       "12   12                            1.0\n",
       "13   13                            1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the accuracy_score_Tensorflow function to get an array of accuracies\n",
    "tensorflow_accuracy = accuracy_score_tensorflow(df)\n",
    "tensorflow_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>Logistic Regression Accuracy - Unscaled</th>\n",
       "      <th>Logistic Regeression Accuracy - Scaled</th>\n",
       "      <th>Log Reg - Unscaled - drop 40% to 60%</th>\n",
       "      <th>Log Reg - Scaled - drop 40% to 60%</th>\n",
       "      <th>Tensorflow Accuracy - Unscaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   week Logistic Regression Accuracy - Unscaled  \\\n",
       "3     3                                     0.6   \n",
       "4     4                                  0.6875   \n",
       "5     5                                     0.5   \n",
       "6     6                                0.857143   \n",
       "7     7                                0.769231   \n",
       "8     8                                0.466667   \n",
       "9     9                                0.642857   \n",
       "10   10                                     0.5   \n",
       "11   11                                0.666667   \n",
       "12   12                                0.666667   \n",
       "13   13                                0.571429   \n",
       "\n",
       "   Logistic Regeression Accuracy - Scaled  \\\n",
       "3                                0.466667   \n",
       "4                                  0.6875   \n",
       "5                                   0.625   \n",
       "6                                0.857143   \n",
       "7                                0.769231   \n",
       "8                                0.466667   \n",
       "9                                0.714286   \n",
       "10                                    0.5   \n",
       "11                                    0.6   \n",
       "12                                    0.6   \n",
       "13                               0.571429   \n",
       "\n",
       "   Log Reg - Unscaled - drop 40% to 60% Log Reg - Scaled - drop 40% to 60%  \\\n",
       "3                              0.538462                           0.555556   \n",
       "4                              0.666667                           0.692308   \n",
       "5                              0.466667                           0.666667   \n",
       "6                              0.833333                           0.909091   \n",
       "7                                  0.75                                0.8   \n",
       "8                                   0.5                           0.466667   \n",
       "9                                   0.6                           0.636364   \n",
       "10                                  0.5                           0.538462   \n",
       "11                             0.777778                           0.777778   \n",
       "12                                  0.8                           0.714286   \n",
       "13                                  0.6                                0.6   \n",
       "\n",
       "   Tensorflow Accuracy - Unscaled  \n",
       "3                             0.4  \n",
       "4                          0.4375  \n",
       "5                           0.375  \n",
       "6                        0.571429  \n",
       "7                        0.692308  \n",
       "8                        0.666667  \n",
       "9                        0.285714  \n",
       "10                            0.5  \n",
       "11                            1.0  \n",
       "12                            1.0  \n",
       "13                            1.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df = pd.concat(\n",
    "    [logistic_regression_accuracy, tensorflow_accuracy.drop(columns=['week'])], axis=1)\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_week = 11\n",
    "comp_games_df = df[df['week'] < pred_week]\n",
    "pred_games_df = df[df['week'] == pred_week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = comp_games_df\n",
    "test_df = pred_games_df\n",
    "\n",
    "X_train = train_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "y_train = train_df[['result']] \n",
    "X_test = test_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "y_test = test_df[['result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The New England Patriots have a probability of 67% of beating the Atlanta Falcons.\n",
      "The Washington Football Team have a probability of 55% of beating the Carolina Panthers.\n",
      "The Indianapolis Colts have a probability of 36% of beating the Buffalo Bills.\n",
      "The New Orleans Saints have a probability of 45% of beating the Philadelphia Eagles.\n",
      "The Houston Texans have a probability of 26% of beating the Tennessee Titans.\n",
      "The Miami Dolphins have a probability of 55% of beating the New York Jets.\n",
      "The Green Bay Packers have a probability of 63% of beating the Minnesota Vikings.\n",
      "The San Francisco 49ers have a probability of 77% of beating the Jacksonville Jaguars.\n",
      "The Detroit Lions have a probability of 12% of beating the Cleveland Browns.\n",
      "The Baltimore Ravens have a probability of 67% of beating the Chicago Bears.\n",
      "The Cincinnati Bengals have a probability of 72% of beating the Las Vegas Raiders.\n",
      "The Arizona Cardinals have a probability of 65% of beating the Seattle Seahawks.\n",
      "The Dallas Cowboys have a probability of 45% of beating the Kansas City Chiefs.\n",
      "The Pittsburgh Steelers have a probability of 40% of beating the Los Angeles Chargers.\n",
      "The New York Giants have a probability of 25% of beating the Tampa Bay Buccaneers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "\n",
    "# Fitting the model\n",
    "rf_fitted = rf_model.fit(X_train, np.ravel(y_train.values))\n",
    "\n",
    "# Making predictions using the testing data\n",
    "y_pred = rf_fitted.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]\n",
    "y = rf_fitted.predict(X_test)\n",
    "\n",
    "# print(\"RandomForest - Unscaled \\n\")\n",
    "display(y_pred, test_df)\n",
    "accuracy_score(y_test, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a for loop to the get accuracy for each week\n",
    "def accuracy_score_random_forest(df):\n",
    "    \n",
    "    accuracy = pd.DataFrame(columns=['week', 'Random Forest Accuracy', 'Random Forest - drop 40% - 60%'])\n",
    "    \n",
    "    for w in df['week'].unique()[1:-1]:\n",
    "        train_df = df[df['week'] < w]\n",
    "        test_df = df[df['week'] == w]\n",
    "        \n",
    "        X_train = train_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "        y_train = train_df[['result']] \n",
    "        X_test = test_df.drop(columns = ['away_name', 'away_abbr', 'home_name', 'home_abbr', 'week','result'])\n",
    "        y_test = test_df[['result']]\n",
    "        \n",
    "        # Fitting the model\n",
    "        classifier = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "        classifier.fit(X_train, np.ravel(y_train.values))\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        y_pred_percent =  classifier.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        model_accuracy = accuracy_score(y_test, y_pred)\n",
    "        model_accuracy_drop_40_60 = accuracy_score(y_test[(y_pred_percent < .4) | (y_pred_percent > .6)], y_pred[(y_pred_percent < .4) | (y_pred_percent > .6)])\n",
    "\n",
    "        # Assign the accuracy to \n",
    "        accuracy.loc[w,:] = [w, model_accuracy, model_accuracy_drop_40_60]\n",
    "        \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>Random Forest Accuracy</th>\n",
       "      <th>Random Forest - drop 40% - 60%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   week Random Forest Accuracy Random Forest - drop 40% - 60%\n",
       "3     3                    0.6                           0.75\n",
       "4     4                  0.625                            0.7\n",
       "5     5                  0.625                            0.8\n",
       "6     6               0.785714                       0.833333\n",
       "7     7               0.846154                            0.8\n",
       "8     8                    0.6                       0.583333\n",
       "9     9               0.642857                            0.6\n",
       "10   10               0.642857                       0.545455\n",
       "11   11                    0.8                            0.7\n",
       "12   12               0.466667                       0.583333\n",
       "13   13               0.642857                       0.777778"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_accuracy = accuracy_score_random_forest(df)\n",
    "random_forest_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>Logistic Regression Accuracy - Unscaled</th>\n",
       "      <th>Logistic Regeression Accuracy - Scaled</th>\n",
       "      <th>Log Reg - Unscaled - drop 40% to 60%</th>\n",
       "      <th>Log Reg - Scaled - drop 40% to 60%</th>\n",
       "      <th>Tensorflow Accuracy - Unscaled</th>\n",
       "      <th>Random Forest Accuracy</th>\n",
       "      <th>Random Forest - drop 40% - 60%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   week Logistic Regression Accuracy - Unscaled  \\\n",
       "3     3                                     0.6   \n",
       "4     4                                  0.6875   \n",
       "5     5                                     0.5   \n",
       "6     6                                0.857143   \n",
       "7     7                                0.769231   \n",
       "8     8                                0.466667   \n",
       "9     9                                0.642857   \n",
       "10   10                                     0.5   \n",
       "11   11                                0.666667   \n",
       "12   12                                0.666667   \n",
       "13   13                                0.571429   \n",
       "\n",
       "   Logistic Regeression Accuracy - Scaled  \\\n",
       "3                                0.466667   \n",
       "4                                  0.6875   \n",
       "5                                   0.625   \n",
       "6                                0.857143   \n",
       "7                                0.769231   \n",
       "8                                0.466667   \n",
       "9                                0.714286   \n",
       "10                                    0.5   \n",
       "11                                    0.6   \n",
       "12                                    0.6   \n",
       "13                               0.571429   \n",
       "\n",
       "   Log Reg - Unscaled - drop 40% to 60% Log Reg - Scaled - drop 40% to 60%  \\\n",
       "3                              0.538462                           0.555556   \n",
       "4                              0.666667                           0.692308   \n",
       "5                              0.466667                           0.666667   \n",
       "6                              0.833333                           0.909091   \n",
       "7                                  0.75                                0.8   \n",
       "8                                   0.5                           0.466667   \n",
       "9                                   0.6                           0.636364   \n",
       "10                                  0.5                           0.538462   \n",
       "11                             0.777778                           0.777778   \n",
       "12                                  0.8                           0.714286   \n",
       "13                                  0.6                                0.6   \n",
       "\n",
       "   Tensorflow Accuracy - Unscaled Random Forest Accuracy  \\\n",
       "3                             0.4                    0.6   \n",
       "4                          0.4375                  0.625   \n",
       "5                           0.375                  0.625   \n",
       "6                        0.571429               0.785714   \n",
       "7                        0.692308               0.846154   \n",
       "8                        0.666667                    0.6   \n",
       "9                        0.285714               0.642857   \n",
       "10                            0.5               0.642857   \n",
       "11                            1.0                    0.8   \n",
       "12                            1.0               0.466667   \n",
       "13                            1.0               0.642857   \n",
       "\n",
       "   Random Forest - drop 40% - 60%  \n",
       "3                            0.75  \n",
       "4                             0.7  \n",
       "5                             0.8  \n",
       "6                        0.833333  \n",
       "7                             0.8  \n",
       "8                        0.583333  \n",
       "9                             0.6  \n",
       "10                       0.545455  \n",
       "11                            0.7  \n",
       "12                       0.583333  \n",
       "13                       0.777778  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df = pd.concat(\n",
    "    [logistic_regression_accuracy, \n",
    "     tensorflow_accuracy.drop(columns=['week']),\n",
    "     random_forest_accuracy.drop(columns=['week'])], \n",
    "    axis=1)\n",
    "\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df.to_csv('Resources/accuracy_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "week                                             88\n",
       "Logistic Regression Accuracy - Unscaled    6.928159\n",
       "Logistic Regeression Accuracy - Scaled     6.857921\n",
       "Log Reg - Unscaled - drop 40% to 60%       7.032906\n",
       "Log Reg - Scaled - drop 40% to 60%         7.357176\n",
       "Tensorflow Accuracy - Unscaled             6.928617\n",
       "Random Forest Accuracy                     7.277106\n",
       "Random Forest - drop 40% - 60%             7.673232\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='2082'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"b4208962-91e5-41cc-95d5-5bfdce264a7b\" data-root-id=\"2082\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    var docs_json = {\"fe3870b7-b186-4e81-85f8-c383c6c1ba8b\":{\"defs\":[{\"extends\":null,\"module\":null,\"name\":\"ReactiveHTML1\",\"overrides\":[],\"properties\":[]},{\"extends\":null,\"module\":null,\"name\":\"FlexBox1\",\"overrides\":[],\"properties\":[{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_content\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_items\"},{\"default\":\"row\",\"kind\":null,\"name\":\"flex_direction\"},{\"default\":\"wrap\",\"kind\":null,\"name\":\"flex_wrap\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"justify_content\"}]},{\"extends\":null,\"module\":null,\"name\":\"TemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]},{\"extends\":null,\"module\":null,\"name\":\"MaterialTemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]}],\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"2114\",\"type\":\"ResetTool\"},{\"attributes\":{\"label\":{\"value\":\"Log Reg - Unscaled - drop 40% to 60%\"},\"renderers\":[{\"id\":\"2178\"}]},\"id\":\"2194\",\"type\":\"LegendItem\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#e5ae38\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2177\",\"type\":\"Line\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#e5ae38\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2176\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"2191\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#30a2da\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2134\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"2172\"}},\"id\":\"2179\",\"type\":\"CDSView\"},{\"attributes\":{\"line_color\":\"#6d904f\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2221\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"2144\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#6d904f\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2200\",\"type\":\"Line\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"2222\"},\"glyph\":{\"id\":\"2225\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"2227\"},\"nonselection_glyph\":{\"id\":\"2226\"},\"selection_glyph\":{\"id\":\"2249\"},\"view\":{\"id\":\"2229\"}},\"id\":\"2228\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"label\":{\"value\":\"Random Forest Accuracy\"},\"renderers\":[{\"id\":\"2256\"}]},\"id\":\"2278\",\"type\":\"LegendItem\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#8b8b8b\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2226\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"2222\"}},\"id\":\"2229\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"2127\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"2251\",\"type\":\"Selection\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"2172\"},\"glyph\":{\"id\":\"2175\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"2177\"},\"nonselection_glyph\":{\"id\":\"2176\"},\"selection_glyph\":{\"id\":\"2195\"},\"view\":{\"id\":\"2179\"}},\"id\":\"2178\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_color\":\"#30a2da\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2149\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":\"#17becf\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2279\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":\"#8b8b8b\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2225\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"2245\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"label\":{\"value\":\"Logistic Regression Accuracy - Unscaled\"},\"renderers\":[{\"id\":\"2135\"}]},\"id\":\"2148\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"2223\",\"type\":\"Selection\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"2150\"},\"glyph\":{\"id\":\"2153\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"2155\"},\"nonselection_glyph\":{\"id\":\"2154\"},\"selection_glyph\":{\"id\":\"2171\"},\"view\":{\"id\":\"2157\"}},\"id\":\"2156\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"2250\"},\"glyph\":{\"id\":\"2253\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"2255\"},\"nonselection_glyph\":{\"id\":\"2254\"},\"selection_glyph\":{\"id\":\"2279\"},\"view\":{\"id\":\"2257\"}},\"id\":\"2256\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"label\":{\"value\":\"Tensorflow Accuracy - Unscaled\"},\"renderers\":[{\"id\":\"2228\"}]},\"id\":\"2248\",\"type\":\"LegendItem\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"2280\"},\"glyph\":{\"id\":\"2283\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"2285\"},\"nonselection_glyph\":{\"id\":\"2284\"},\"selection_glyph\":{\"id\":\"2311\"},\"view\":{\"id\":\"2287\"}},\"id\":\"2286\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#8b8b8b\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2227\",\"type\":\"Line\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#fc4f30\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2154\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":\"#e5ae38\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2175\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"2280\"}},\"id\":\"2287\",\"type\":\"CDSView\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"HSpacer04236\",\"sizing_mode\":\"stretch_width\"},\"id\":\"2083\",\"type\":\"Spacer\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#9467bd\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2284\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"2150\"}},\"id\":\"2157\",\"type\":\"CDSView\"},{\"attributes\":{\"line_color\":\"#9467bd\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2283\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":\"#fc4f30\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2153\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"2307\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"2125\",\"type\":\"AllLabels\"},{\"attributes\":{\"label\":{\"value\":\"Random Forest - drop 40% - 60%\"},\"renderers\":[{\"id\":\"2286\"}]},\"id\":\"2310\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"2167\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"2124\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"data\":{\"Variable\":[\"Log Reg - Unscaled - drop 40% to 60%\",\"Log Reg - Unscaled - drop 40% to 60%\",\"Log Reg - Unscaled - drop 40% to 60%\",\"Log Reg - Unscaled - drop 40% to 60%\",\"Log Reg - Unscaled - drop 40% to 60%\",\"Log Reg - Unscaled - drop 40% to 60%\",\"Log Reg - Unscaled - drop 40% to 60%\",\"Log Reg - Unscaled - drop 40% to 60%\",\"Log Reg - Unscaled - drop 40% to 60%\",\"Log Reg - Unscaled - drop 40% to 60%\",\"Log Reg - Unscaled - drop 40% to 60%\"],\"value\":{\"__ndarray__\":\"sRM7sRM74T9VVVVVVVXlP97d3d3d3d0/q6qqqqqq6j8AAAAAAADoPwAAAAAAAOA/MzMzMzMz4z8AAAAAAADgPzmO4ziO4+g/mpmZmZmZ6T8zMzMzMzPjPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[11]},\"week\":[3,4,5,6,7,8,9,10,11,12,13]},\"selected\":{\"id\":\"2173\"},\"selection_policy\":{\"id\":\"2191\"}},\"id\":\"2172\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"2281\",\"type\":\"Selection\"},{\"attributes\":{\"line_color\":\"#e5ae38\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2195\",\"type\":\"Line\"},{\"attributes\":{\"data\":{\"Variable\":[\"Logistic Regeression Accuracy - Scaled\",\"Logistic Regeression Accuracy - Scaled\",\"Logistic Regeression Accuracy - Scaled\",\"Logistic Regeression Accuracy - Scaled\",\"Logistic Regeression Accuracy - Scaled\",\"Logistic Regeression Accuracy - Scaled\",\"Logistic Regeression Accuracy - Scaled\",\"Logistic Regeression Accuracy - Scaled\",\"Logistic Regeression Accuracy - Scaled\",\"Logistic Regeression Accuracy - Scaled\",\"Logistic Regeression Accuracy - Scaled\"],\"value\":{\"__ndarray__\":\"3t3d3d3d3T8AAAAAAADmPwAAAAAAAOQ/27Zt27Zt6z/ZiZ3YiZ3oP97d3d3d3d0/t23btm3b5j8AAAAAAADgPzMzMzMzM+M/MzMzMzMz4z+SJEmSJEniPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[11]},\"week\":[3,4,5,6,7,8,9,10,11,12,13]},\"selected\":{\"id\":\"2151\"},\"selection_policy\":{\"id\":\"2167\"}},\"id\":\"2150\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"2128\",\"type\":\"AllLabels\"},{\"attributes\":{\"line_color\":\"#9467bd\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2311\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"2151\",\"type\":\"Selection\"},{\"attributes\":{\"end\":13.0,\"reset_end\":13.0,\"reset_start\":3.0,\"start\":3.0,\"tags\":[[[\"week\",\"week\",null]]]},\"id\":\"2084\",\"type\":\"Range1d\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#9467bd\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2285\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":\"#30a2da\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2132\",\"type\":\"Line\"},{\"attributes\":{\"label\":{\"value\":\"Logistic Regeression Accuracy - Scaled\"},\"renderers\":[{\"id\":\"2156\"}]},\"id\":\"2170\",\"type\":\"LegendItem\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#fc4f30\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2155\",\"type\":\"Line\"},{\"attributes\":{\"end\":1.071428570151329,\"reset_end\":1.071428570151329,\"reset_start\":0.21428572833538057,\"start\":0.21428572833538057,\"tags\":[[[\"value\",\"value\",null]]]},\"id\":\"2085\",\"type\":\"Range1d\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#6d904f\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2201\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"2173\",\"type\":\"Selection\"},{\"attributes\":{\"callback\":null,\"renderers\":[{\"id\":\"2135\"},{\"id\":\"2156\"},{\"id\":\"2178\"},{\"id\":\"2202\"},{\"id\":\"2228\"},{\"id\":\"2256\"},{\"id\":\"2286\"}],\"tags\":[\"hv_created\"],\"tooltips\":[[\"Variable\",\"@{Variable}\"],[\"week\",\"@{week}\"],[\"value\",\"@{value}\"]]},\"id\":\"2086\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"2098\",\"type\":\"LinearScale\"},{\"attributes\":{\"data\":{\"Variable\":[\"Random Forest - drop 40% - 60%\",\"Random Forest - drop 40% - 60%\",\"Random Forest - drop 40% - 60%\",\"Random Forest - drop 40% - 60%\",\"Random Forest - drop 40% - 60%\",\"Random Forest - drop 40% - 60%\",\"Random Forest - drop 40% - 60%\",\"Random Forest - drop 40% - 60%\",\"Random Forest - drop 40% - 60%\",\"Random Forest - drop 40% - 60%\",\"Random Forest - drop 40% - 60%\"],\"value\":{\"__ndarray__\":\"AAAAAAAA6D9mZmZmZmbmP5qZmZmZmek/q6qqqqqq6j+amZmZmZnpP6uqqqqqquI/MzMzMzMz4z900UUXXXThP2ZmZmZmZuY/q6qqqqqq4j85juM4juPoPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[11]},\"week\":[3,4,5,6,7,8,9,10,11,12,13]},\"selected\":{\"id\":\"2281\"},\"selection_policy\":{\"id\":\"2307\"}},\"id\":\"2280\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"2196\"},\"glyph\":{\"id\":\"2199\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"2201\"},\"nonselection_glyph\":{\"id\":\"2200\"},\"selection_glyph\":{\"id\":\"2221\"},\"view\":{\"id\":\"2203\"}},\"id\":\"2202\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"data\":{\"Variable\":[\"Logistic Regression Accuracy - Unscaled\",\"Logistic Regression Accuracy - Unscaled\",\"Logistic Regression Accuracy - Unscaled\",\"Logistic Regression Accuracy - Unscaled\",\"Logistic Regression Accuracy - Unscaled\",\"Logistic Regression Accuracy - Unscaled\",\"Logistic Regression Accuracy - Unscaled\",\"Logistic Regression Accuracy - Unscaled\",\"Logistic Regression Accuracy - Unscaled\",\"Logistic Regression Accuracy - Unscaled\",\"Logistic Regression Accuracy - Unscaled\"],\"value\":{\"__ndarray__\":\"MzMzMzMz4z8AAAAAAADmPwAAAAAAAOA/27Zt27Zt6z/ZiZ3YiZ3oP97d3d3d3d0/JUmSJEmS5D8AAAAAAADgP1VVVVVVVeU/VVVVVVVV5T+SJEmSJEniPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[11]},\"week\":[3,4,5,6,7,8,9,10,11,12,13]},\"selected\":{\"id\":\"2130\"},\"selection_policy\":{\"id\":\"2144\"}},\"id\":\"2129\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"children\":[{\"id\":\"2083\"},{\"id\":\"2093\"},{\"id\":\"2510\"}],\"margin\":[0,0,0,0],\"name\":\"Row04232\",\"tags\":[\"embedded\"]},\"id\":\"2082\",\"type\":\"Row\"},{\"attributes\":{\"line_color\":\"#6d904f\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2199\",\"type\":\"Line\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#17becf\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2254\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"2217\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"source\":{\"id\":\"2196\"}},\"id\":\"2203\",\"type\":\"CDSView\"},{\"attributes\":{\"click_policy\":\"mute\",\"coordinates\":null,\"group\":null,\"items\":[{\"id\":\"2148\"},{\"id\":\"2170\"},{\"id\":\"2194\"},{\"id\":\"2220\"},{\"id\":\"2248\"},{\"id\":\"2278\"},{\"id\":\"2310\"}],\"location\":[0,0],\"title\":\"Variable\"},\"id\":\"2147\",\"type\":\"Legend\"},{\"attributes\":{\"line_color\":\"#8b8b8b\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2249\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"2197\",\"type\":\"Selection\"},{\"attributes\":{\"data\":{\"Variable\":[\"Random Forest Accuracy\",\"Random Forest Accuracy\",\"Random Forest Accuracy\",\"Random Forest Accuracy\",\"Random Forest Accuracy\",\"Random Forest Accuracy\",\"Random Forest Accuracy\",\"Random Forest Accuracy\",\"Random Forest Accuracy\",\"Random Forest Accuracy\",\"Random Forest Accuracy\"],\"value\":{\"__ndarray__\":\"MzMzMzMz4z8AAAAAAADkPwAAAAAAAOQ/SZIkSZIk6T87sRM7sRPrPzMzMzMzM+M/JUmSJEmS5D8lSZIkSZLkP5qZmZmZmek/3t3d3d3d3T8lSZIkSZLkPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[11]},\"week\":[3,4,5,6,7,8,9,10,11,12,13]},\"selected\":{\"id\":\"2251\"},\"selection_policy\":{\"id\":\"2275\"}},\"id\":\"2250\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data\":{\"Variable\":[\"Tensorflow Accuracy - Unscaled\",\"Tensorflow Accuracy - Unscaled\",\"Tensorflow Accuracy - Unscaled\",\"Tensorflow Accuracy - Unscaled\",\"Tensorflow Accuracy - Unscaled\",\"Tensorflow Accuracy - Unscaled\",\"Tensorflow Accuracy - Unscaled\",\"Tensorflow Accuracy - Unscaled\",\"Tensorflow Accuracy - Unscaled\",\"Tensorflow Accuracy - Unscaled\",\"Tensorflow Accuracy - Unscaled\"],\"value\":{\"__ndarray__\":\"AAAAoJmZ2T8AAAAAAADcPwAAAAAAANg/AAAAoCRJ4j8AAACAYifmPwAAAGBVVeU/AAAAoCRJ0j8AAAAAAADgPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[11]},\"week\":[3,4,5,6,7,8,9,10,11,12,13]},\"selected\":{\"id\":\"2223\"},\"selection_policy\":{\"id\":\"2245\"}},\"id\":\"2222\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"label\":{\"value\":\"Log Reg - Scaled - drop 40% to 60%\"},\"renderers\":[{\"id\":\"2202\"}]},\"id\":\"2220\",\"type\":\"LegendItem\"},{\"attributes\":{\"source\":{\"id\":\"2129\"}},\"id\":\"2136\",\"type\":\"CDSView\"},{\"attributes\":{\"axis_label\":\"week\",\"coordinates\":null,\"formatter\":{\"id\":\"2124\"},\"group\":null,\"major_label_policy\":{\"id\":\"2125\"},\"ticker\":{\"id\":\"2103\"}},\"id\":\"2102\",\"type\":\"LinearAxis\"},{\"attributes\":{\"coordinates\":null,\"group\":null,\"text_color\":\"black\",\"text_font_size\":\"12pt\"},\"id\":\"2094\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"2100\",\"type\":\"LinearScale\"},{\"attributes\":{\"source\":{\"id\":\"2250\"}},\"id\":\"2257\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"2103\",\"type\":\"BasicTicker\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#30a2da\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2133\",\"type\":\"Line\"},{\"attributes\":{\"axis\":{\"id\":\"2102\"},\"coordinates\":null,\"grid_line_color\":null,\"group\":null,\"ticker\":null},\"id\":\"2105\",\"type\":\"Grid\"},{\"attributes\":{\"below\":[{\"id\":\"2102\"}],\"center\":[{\"id\":\"2105\"},{\"id\":\"2109\"}],\"height\":500,\"left\":[{\"id\":\"2106\"}],\"margin\":[5,5,5,5],\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"min_border_top\":10,\"renderers\":[{\"id\":\"2135\"},{\"id\":\"2156\"},{\"id\":\"2178\"},{\"id\":\"2202\"},{\"id\":\"2228\"},{\"id\":\"2256\"},{\"id\":\"2286\"}],\"right\":[{\"id\":\"2147\"}],\"sizing_mode\":\"fixed\",\"title\":{\"id\":\"2094\"},\"toolbar\":{\"id\":\"2116\"},\"width\":1000,\"x_range\":{\"id\":\"2084\"},\"x_scale\":{\"id\":\"2098\"},\"y_range\":{\"id\":\"2085\"},\"y_scale\":{\"id\":\"2100\"}},\"id\":\"2093\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"line_color\":\"#17becf\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2253\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"2130\",\"type\":\"Selection\"},{\"attributes\":{\"axis\":{\"id\":\"2106\"},\"coordinates\":null,\"dimension\":1,\"grid_line_color\":null,\"group\":null,\"ticker\":null},\"id\":\"2109\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"2275\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#17becf\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2255\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"2111\",\"type\":\"PanTool\"},{\"attributes\":{\"line_color\":\"#fc4f30\",\"line_width\":2,\"x\":{\"field\":\"week\"},\"y\":{\"field\":\"value\"}},\"id\":\"2171\",\"type\":\"Line\"},{\"attributes\":{\"axis_label\":\"\",\"coordinates\":null,\"formatter\":{\"id\":\"2127\"},\"group\":null,\"major_label_policy\":{\"id\":\"2128\"},\"ticker\":{\"id\":\"2107\"}},\"id\":\"2106\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"2107\",\"type\":\"BasicTicker\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"2129\"},\"glyph\":{\"id\":\"2132\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"2134\"},\"nonselection_glyph\":{\"id\":\"2133\"},\"selection_glyph\":{\"id\":\"2149\"},\"view\":{\"id\":\"2136\"}},\"id\":\"2135\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"2112\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"HSpacer04237\",\"sizing_mode\":\"stretch_width\"},\"id\":\"2510\",\"type\":\"Spacer\"},{\"attributes\":{\"tools\":[{\"id\":\"2086\"},{\"id\":\"2110\"},{\"id\":\"2111\"},{\"id\":\"2112\"},{\"id\":\"2113\"},{\"id\":\"2114\"}]},\"id\":\"2116\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"2110\",\"type\":\"SaveTool\"},{\"attributes\":{\"overlay\":{\"id\":\"2115\"}},\"id\":\"2113\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"data\":{\"Variable\":[\"Log Reg - Scaled - drop 40% to 60%\",\"Log Reg - Scaled - drop 40% to 60%\",\"Log Reg - Scaled - drop 40% to 60%\",\"Log Reg - Scaled - drop 40% to 60%\",\"Log Reg - Scaled - drop 40% to 60%\",\"Log Reg - Scaled - drop 40% to 60%\",\"Log Reg - Scaled - drop 40% to 60%\",\"Log Reg - Scaled - drop 40% to 60%\",\"Log Reg - Scaled - drop 40% to 60%\",\"Log Reg - Scaled - drop 40% to 60%\",\"Log Reg - Scaled - drop 40% to 60%\"],\"value\":{\"__ndarray__\":\"chzHcRzH4T92Yid2YifmP1VVVVVVVeU/F1100UUX7T+amZmZmZnpP97d3d3d3d0/XXTRRRdd5D+xEzuxEzvhPzmO4ziO4+g/t23btm3b5j8zMzMzMzPjPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[11]},\"week\":[3,4,5,6,7,8,9,10,11,12,13]},\"selected\":{\"id\":\"2197\"},\"selection_policy\":{\"id\":\"2217\"}},\"id\":\"2196\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"bottom_units\":\"screen\",\"coordinates\":null,\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"group\":null,\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"2115\",\"type\":\"BoxAnnotation\"}],\"root_ids\":[\"2082\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.1\"}};\n",
       "    var render_items = [{\"docid\":\"fe3870b7-b186-4e81-85f8-c383c6c1ba8b\",\"root_ids\":[\"2082\"],\"roots\":{\"2082\":\"b4208962-91e5-41cc-95d5-5bfdce264a7b\"}}];\n",
       "    root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":NdOverlay   [Variable]\n",
       "   :Curve   [week]   (value)"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "2082"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot the accuracy dataframe\n",
    "accuracy_df.hvplot(x='week', width=1000, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='tableauPlaceholder' id='viz1638998595666' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NF&#47;NFLPredictionModelAccuracy&#47;NFLPredictionModelAccuracy&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='NFLPredictionModelAccuracy&#47;NFLPredictionModelAccuracy' /><param name='tabs' value='yes' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NF&#47;NFLPredictionModelAccuracy&#47;NFLPredictionModelAccuracy&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-US' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1638998595666');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='800px';vizElement.style.maxWidth='100%';vizElement.style.minHeight='1250px';vizElement.style.maxHeight=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='800px';vizElement.style.maxWidth='100%';vizElement.style.minHeight='1250px';vizElement.style.maxHeight=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.minHeight='800px';vizElement.style.maxHeight=(divElement.offsetWidth*1.77)+'px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<div class='tableauPlaceholder' id='viz1638998595666' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NF&#47;NFLPredictionModelAccuracy&#47;NFLPredictionModelAccuracy&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='NFLPredictionModelAccuracy&#47;NFLPredictionModelAccuracy' /><param name='tabs' value='yes' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NF&#47;NFLPredictionModelAccuracy&#47;NFLPredictionModelAccuracy&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-US' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1638998595666');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.minWidth='800px';vizElement.style.maxWidth='100%';vizElement.style.minHeight='1250px';vizElement.style.maxHeight=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.minWidth='800px';vizElement.style.maxWidth='100%';vizElement.style.minHeight='1250px';vizElement.style.maxHeight=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.minHeight='800px';vizElement.style.maxHeight=(divElement.offsetWidth*1.77)+'px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
